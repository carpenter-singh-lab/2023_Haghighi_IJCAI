"""
[The base of this script is Matterport implementation of Mask R-CNN model Written by Waleed Abdulla (Copyright (c) 2017 Matterport, Inc.).]


Data Generator functions are very important in our model since we have huge number of objects and handling data is important

List of generators and their properties:


------------------------------------------------------


* DataGenerator (class)
  - def __init__(self,dataset_inp, config, shuffle=False, augment=False, augmentation=None,
                   random_rois=0, batch_size=1, detection_targets=False,
                   no_augmentation_sources=None):
  - def __len__(self):
  - def __getitem__(self, index):
    which calls:
    - def __data_generation(self, dataset, batch_image_ids):
      - def load_image_gt(dataset, config, image_id, augment=False, augmentation=None,use_mini_mask=False)
               -  image = dataset.load_image_cr_presaved(image_id,config.batchplate_well) or\
                  image = dataset.load_image(image_id)
               -  mask, class_ids = dataset.load_mask(image_id) or mask, class_ids = dataset.create_mask(image_id,image)
               - replaced with: bbox, class_ids = dataset.load_bbox(image_id)
         return image, image_meta, class_ids, bbox, mask
      
      - def build_rpn_targets(image_shape, anchors, gt_class_ids, gt_boxes, config)
          return rpn_match, rpn_bbox, rpn_match_class
      
      
      - def mold_image(images, config)
      
  - def on_epoch_end(self):
  
  -> Operation algorithm:
     This data generator is designed so that it goes through images of one site in each epoch,
     and in each step or iteration inside each epoch it goes through all the cropped sequences of 
     that site (n=484 max if crops are 256x256 - image_ids 484*9=4356 max) 
     Therefore when using this data generator, epochs must be more than number of sites in each well 
     and STEPS_PER_EPOCH should be more than number of cropped sequences (here 484).
     We dont select sites randomly from the dataset_inp list since that way we need MANY epochs to 
     ensure going through the whole list of sites and generate barcodes for all!



* DataGenerator_test (class)



* DataGeneratorBatch (class)  --- is called when -> config.load_batch_images = True
  This class is for when we want to create locations on the fly by thresholding the max-projected 
  image. We need another generator which takes all the 9 cycles images together for creating the gt_ 
  variables in each batch.
  
  Here, __data_generation is differnt than DataGenerator class. 
  - def __data_generation(self, dataset, batch_image_ids):
      - def load_batch_ims_cr_presaved(self, image_id,batchplate_well):
      - def create_mask_batch(self,images4D):

 

------------------------------------------------------


* data_generator (function)
  
* data_generator_test (function)


Author: 
Marzieh Haghighi
"""


import math
import pdb

import numpy as np
import tensorflow.keras as keras
import tensorflow.keras.utils as KU

import barcodefit.model.model_utils as utils

seed = 42

# Set seed for NumPy
np.random.seed(seed)


# -----------------------------------------------------------------------------------------
# from tensorflow.keras.utils import Sequence
class DataGenerator(KU.Sequence):
    """Generates data for Keras Sequence based data generator.
    Suitable for building data generator for training and prediction.
    """

    def __init__(
        self,
        dataset_inp,
        config,
        shuffle=False,
        augment=False,
        augmentation=None,
        random_rois=0,
        true_rois=False,
        batch_size=1,
        detection_targets=False,
        no_augmentation_sources=None,
    ):

        """Initialization
        dataset_inp: is a list of dataobjects for all sites -> len=number of sites
        dataset: The Dataset object to pick data from
        config: The model config object
        shuffle: If True, shuffles the samples before every epoch
        augment: (deprecated. Use augmentation instead). If true, apply random
            image augmentation. Currently, only horizontal flipping is offered.
        augmentation: Optional. An imgaug (https://github.com/aleju/imgaug) augmentation.
            For example, passing imgaug.augmenters.Fliplr(0.5) flips images
            right/left 50% of the time.
        random_rois: If > 0 then generate proposals to be used to train the
                     network classifier and mask heads. Useful if training
                     the Mask RCNN part without the RPN.
        batch_size: How many images to return in each call
        detection_targets: If True, generate detection targets (class IDs, bbox
            deltas, and masks). Typically for debugging or visualizations because
            in trainig detection targets are generated by DetectionTargetLayer.
        no_augmentation_sources: Optional. List of sources to exclude for
            augmentation. A source is string that identifies a dataset and is
            defined in the Dataset class.
        """

        if not config.USE_RPN_ROIS:
            self.true_rois = config.POST_NMS_ROIS_TRAINING
        else:
            self.true_rois = true_rois

        self.epoch = -1
        self.site_step = config.starting_site  # -1#20
        self.dataset_inp = dataset_inp
        self.n_total_sites = len(dataset_inp)
        self.config = config
        self.augment = augment
        self.augmentation = augmentation
        self.random_rois = random_rois
        #         self.random_rois = 500  #changed by Marzi
        self.batch_size = batch_size
        self.detection_targets = detection_targets
        self.no_augmentation_sources = no_augmentation_sources or []

        self.indexes = np.arange(0, len(dataset_inp[0].image_ids), self.batch_size)
        # Anchors
        # [anchor_count, (y1, x1, y2, x2)]
        self.backbone_shapes = compute_backbone_shapes(
            self.config, self.config.IMAGE_SHAPE
        )
        print("self.backbone_shapes", self.backbone_shapes)
        self.anchors = utils.generate_pyramid_anchors(
            self.config.RPN_ANCHOR_SCALES,
            self.config.RPN_ANCHOR_RATIOS,
            self.backbone_shapes,
            self.config.BACKBONE_STRIDES,
            self.config.RPN_ANCHOR_STRIDE,
        )

        self.on_epoch_end()

    #         self.on_test_end()

    def __len__(self):
        """Denotes the number of batches per epoch
        :return: number of batches per epoch
        """
        return self.config.STEPS_PER_EPOCH

    def __getitem__(self, index):
        """Generate one batch of data

        index: index of the batch
        return: X and y when fitting. X only when predicting
        """
        # Generate indexes of the batch by checking the current site first

        #         dataset=random.choice(dataset_inp); # for when we want to randomly select a site

        #         When site_step or epoch exceeds number of sites in the list of dataset_inp, we again go through
        #         the list from start
        dataset = self.dataset_inp[self.site_step % self.n_total_sites]

        #         print('epoch: ', self.epoch)
        #         print('Site: ', int(dataset.image_info[0]["site"]))

        # image ids for each site which are basically ids for crops
        image_ids = np.copy(dataset.image_ids)

        #         print('image_ids:',image_ids)
        #         print('image_index:',image_index)

        #         print('ind:',index,index*9,(index*9) % len(image_ids))
        #         image_index_firstSeq = (self.indexes[index]) % len(image_ids)

        #         When index exceeds number of cropped sequences in each site, we again go through
        #         the crops from the start
        index = index % len(self.indexes)

        image_index_firstSeq = self.indexes[index]
        #         print('image_index_firstSeq: ', index,image_index_firstSeq)

        #         get the list of image_ids for a cropped sequence
        batch_image_ids = image_ids[
            image_index_firstSeq : image_index_firstSeq + self.batch_size
        ]

        inputs, outputs = self.__data_generation(dataset, batch_image_ids)

        return inputs, outputs

    def __data_generation(self, dataset, batch_image_ids):
        "Generates data containing batch_size samples"  # X : (n_samples, *dim, n_channels)

        batch_size = self.batch_size
        b = 0
        #         print(self.epoch)

        for image_id in batch_image_ids:
            #             print('image_id',image_id)

            #             image, image_meta, gt_class_ids, gt_boxes, gt_masks = \
            #             load_image_gt(dataset, self.config, image_id, augment=self.augment,
            #                           augmentation=None,
            #                           use_mini_mask=self.config.USE_MINI_MASK,epoch=self.epoch)

            image, image_meta, gt_class_ids, gt_boxes = load_image_gt(
                dataset,
                self.config,
                image_id,
                augment=self.augment,
                augmentation=None,
                use_mini_mask=self.config.USE_MINI_MASK,
                epoch=self.epoch,
            )

            #             print(image_meta.shape)

            # Skip images that have no instances. This can happen in cases
            # where we train on a subset of classes and the image doesn't
            # have any of the classes we care about.
            if not np.any(gt_class_ids > 0):
                continue

            # RPN Targets
            if self.config.rpn_clustering:
                rpn_match, rpn_bbox, rpn_match_class = build_rpn_targets_marzi(
                    image.shape, self.anchors, gt_class_ids, gt_boxes, self.config
                )
            #                 print(rpn_match.shape, rpn_bbox.shape, rpn_match_class.shape,gt_class_ids.shape, gt_boxes.shape)
            else:
                rpn_match, rpn_bbox, rpn_match_class = build_rpn_targets(
                    image.shape, self.anchors, gt_class_ids, gt_boxes, self.config
                )

            #             rpn_match, rpn_bbox ,anchors= build_rpn_targets(image.shape, anchors,
            #                                                     gt_class_ids, gt_boxes, config) # added by Marzi

            #             print("rpn_bbox",rpn_bbox.shape)#(96, 4)
            #             print("rpn_match",rpn_bbox.shape) (96, 4)

            # Mask R-CNN Targets
            if self.random_rois:
                rpn_rois = generate_random_rois(
                    image.shape, self.random_rois, gt_class_ids, gt_boxes
                )
                if self.detection_targets:
                    (
                        rois,
                        mrcnn_class_ids,
                        mrcnn_bbox,
                        mrcnn_mask,
                    ) = build_detection_targets(
                        rpn_rois, gt_class_ids, gt_boxes, gt_masks, self.config
                    )

            # Mask R-CNN GT targets
            if self.true_rois:
                rpn_rois = generate_gt_rois(
                    image.shape, self.true_rois, gt_class_ids, gt_boxes
                )

            # Init batch arrays
            if b == 0:
                batch_image_meta = np.zeros(
                    (batch_size,) + image_meta.shape, dtype=image_meta.dtype
                )
                batch_rpn_match = np.zeros(
                    [batch_size, self.anchors.shape[0], 1], dtype=rpn_match.dtype
                )
                batch_rpn_match_class = np.zeros(
                    [batch_size, self.anchors.shape[0], 1], dtype=rpn_match_class.dtype
                )
                if self.config.rpn_clustering:
                    batch_rpn_bbox = np.zeros(
                        [batch_size, rpn_bbox.shape[0], 4], dtype=rpn_bbox.dtype
                    )
                else:
                    batch_rpn_bbox = np.zeros(
                        [batch_size, self.config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4],
                        dtype=rpn_bbox.dtype,
                    )

                batch_images = np.zeros((batch_size,) + image.shape, dtype=np.float32)
                batch_gt_class_ids = np.zeros(
                    (batch_size, self.config.MAX_GT_INSTANCES), dtype=np.int32
                )
                batch_gt_boxes = np.zeros(
                    (batch_size, self.config.MAX_GT_INSTANCES, 4), dtype=np.int32
                )
                #                 batch_gt_masks = np.zeros(
                #                     (batch_size, gt_masks.shape[0], gt_masks.shape[1],
                #                      self.config.MAX_GT_INSTANCES), dtype=gt_masks.dtype)

                if self.true_rois:
                    batch_rpn_rois = np.zeros(
                        (batch_size, rpn_rois.shape[0], 4), dtype=rpn_rois.dtype
                    )

                if self.random_rois:
                    batch_rpn_rois = np.zeros(
                        (batch_size, rpn_rois.shape[0], 4), dtype=rpn_rois.dtype
                    )
                    if self.detection_targets:
                        batch_rois = np.zeros(
                            (batch_size,) + rois.shape, dtype=rois.dtype
                        )
                        batch_mrcnn_class_ids = np.zeros(
                            (batch_size,) + mrcnn_class_ids.shape,
                            dtype=mrcnn_class_ids.dtype,
                        )
                        batch_mrcnn_bbox = np.zeros(
                            (batch_size,) + mrcnn_bbox.shape, dtype=mrcnn_bbox.dtype
                        )
            #                         batch_mrcnn_mask = np.zeros(
            #                             (batch_size,) + mrcnn_mask.shape, dtype=mrcnn_mask.dtype)

            # If more instances than fits in the array, sub-sample from them.
            if gt_boxes.shape[0] > self.config.MAX_GT_INSTANCES:
                #                 ids = np.random.choice(
                #                     np.arange(gt_boxes.shape[0]), config.MAX_GT_INSTANCES, replace=False)
                #                 gt_class_ids = gt_class_ids[ids]
                gt_class_ids = gt_class_ids[0 : self.config.MAX_GT_INSTANCES]
                gt_boxes = gt_boxes[0 : self.config.MAX_GT_INSTANCES]
            #                 gt_masks = gt_masks[:, :, 0:self.config.MAX_GT_INSTANCES]

            # Add to batch
            batch_image_meta[b] = image_meta
            batch_rpn_match[b] = rpn_match[:, np.newaxis]
            batch_rpn_match_class[b] = rpn_match_class[:, np.newaxis]
            if rpn_bbox.shape[0] >= batch_rpn_bbox.shape[1]:
                batch_rpn_bbox[b] = rpn_bbox[: batch_rpn_bbox.shape[1], :]
            else:
                batch_rpn_bbox[b, : rpn_bbox.shape[0], :] = rpn_bbox
            batch_images[b] = mold_image(image.astype(np.float32), self.config)
            batch_gt_class_ids[b, : gt_class_ids.shape[0]] = gt_class_ids
            batch_gt_boxes[b, : gt_boxes.shape[0]] = gt_boxes
            #             batch_gt_masks[b, :, :, :gt_masks.shape[-1]] = gt_masks

            if self.true_rois:
                batch_rpn_rois[b] = rpn_rois

            if self.random_rois:
                batch_rpn_rois[b] = rpn_rois
                if self.detection_targets:
                    batch_rois[b] = rois
                    batch_mrcnn_class_ids[b] = mrcnn_class_ids
                    batch_mrcnn_bbox[b] = mrcnn_bbox
            #                     batch_mrcnn_mask[b] = mrcnn_mask
            b += 1

        # Batch full?
        #         if b >= batch_size:
        #         inputs = [batch_images, batch_image_meta, batch_rpn_match, batch_rpn_bbox,
        #                   batch_gt_class_ids, batch_gt_boxes, batch_gt_masks]

        inputs = [
            batch_images,
            batch_image_meta,
            batch_rpn_match,
            batch_rpn_bbox,
            batch_gt_class_ids,
            batch_gt_boxes,
            batch_rpn_match_class,
        ]
        #                 print("batch_gt_boxes",batch_gt_boxes)
        outputs = []
        if self.true_rois:
            inputs.extend([batch_rpn_rois])

        if self.random_rois:
            inputs.extend([batch_rpn_rois])
            if self.detection_targets:
                inputs.extend([batch_rois])
                # Keras requires that output and targets have the same number of dimensions
                batch_mrcnn_class_ids = np.expand_dims(batch_mrcnn_class_ids, -1)
                outputs.extend([batch_mrcnn_class_ids, batch_mrcnn_bbox])
        #         print("batch_images min max",batch_images.min(),batch_images.max())
        return inputs, outputs

    def on_epoch_end(self):
        """Updates indexes after each epoch"""
        #         self.indexes = np.arange(len(self.list_IDs))
        self.site_step += 1
        self.epoch += 1

        self.indexes = np.arange(
            0,
            len(self.dataset_inp[self.site_step % self.n_total_sites].image_ids),
            self.batch_size,
        )

    def on_test_batch_end(self):
        self.site_step += 1
        self.epoch += 1
        self.indexes = np.arange(
            0,
            len(self.dataset_inp[self.site_step % self.n_total_sites].image_ids),
            self.batch_size,
        )


#     def on_test_end(self):
#         self.site_step+=1
# #         self.epoch+=1
#         print("site_step",self.site_step)
#         self.indexes = np.arange(0,len(self.dataset_inp[self.site_step % self.n_total_sites].image_ids),self.batch_size)

# -----------------------------------------------------------------------------------------
class DataGeneratorTest(KU.Sequence):
    """Generates data for Keras
    Sequence based data generator. Suitable for building data generator for training and prediction.
    """

    def __init__(
        self,
        dataset_inp,
        config,
        shuffle=False,
        augment=False,
        augmentation=None,
        random_rois=0,
        batch_size=1,
        detection_targets=False,
        anchors=None,
    ):

        """
        dataset: The Dataset object to pick data from
        """
        self.epoch = -1
        self.site_step = -1
        self.dataset_inp = dataset_inp
        self.n_total_sites = len(dataset_inp)
        self.config = config
        self.augment = augment
        self.augmentation = augmentation
        self.random_rois = random_rois
        self.batch_size = batch_size
        self.detection_targets = detection_targets
        self.no_augmentation_sources = []

        self.indexes = np.arange(0, len(dataset_inp[0].image_ids), self.batch_size)
        # Anchors
        # [anchor_count, (y1, x1, y2, x2)]
        self.backbone_shapes = compute_backbone_shapes(
            self.config, self.config.IMAGE_SHAPE
        )
        self.anchors = utils.generate_pyramid_anchors(
            self.config.RPN_ANCHOR_SCALES,
            self.config.RPN_ANCHOR_RATIOS,
            self.backbone_shapes,
            self.config.BACKBONE_STRIDES,
            self.config.RPN_ANCHOR_STRIDE,
        )
        #         self.anchors =anchors

        self.on_epoch_end()

    def __len__(self):
        """Denotes the number of batches per epoch
        :return: number of batches per epoch
        """
        return self.config.STEPS_PER_EPOCH

    def __getitem__(self, index):
        """Generate one batch of data
        :param index: index of the batch
        :return: X and y when fitting. X only when predicting
        """
        # Generate indexes of the batch

        #         dataset=random.choice(dataset_inp);
        dataset = self.dataset_inp[self.site_step % self.n_total_sites]
        print("epoch: ", self.epoch)
        print("Site: ", int(dataset.image_info[0]["site"]))

        image_ids = np.copy(dataset.image_ids)
        #         print('image_ids:',image_ids)
        #         print('image_index:',image_index)

        #         print('ind:',index,index*9,(index*9) % len(image_ids))
        #         image_index_firstSeq = (self.indexes[index]) % len(image_ids)
        index = index % len(self.indexes)
        image_index_firstSeq = self.indexes[index]
        #         print('image_index_firstSeq: ', index,image_index_firstSeq)

        batch_image_ids = image_ids[
            image_index_firstSeq : image_index_firstSeq + self.batch_size
        ]

        inputs, outputs = self.__data_generation(dataset, batch_image_ids)

        return inputs, outputs

    def __data_generation(self, dataset, batch_image_ids):
        "Generates data containing batch_size samples"  # X : (n_samples, *dim, n_channels)
        # Initialization

        batch_size = self.batch_size
        b = 0
        for image_id in batch_image_ids:
            #             image_id = image_ids[image_index]
            #             print('image_id',image_id)
            # If the image source is not to be augmented pass None as augmentation
            if dataset.image_info[image_id]["source"] in self.no_augmentation_sources:
                image, image_meta, gt_class_ids, gt_boxes, gt_masks = load_image_gt(
                    dataset,
                    self.config,
                    image_id,
                    augment=self.augment,
                    augmentation=None,
                    use_mini_mask=self.config.USE_MINI_MASK,
                    epoch=self.epoch,
                )

            else:
                image, image_meta, gt_class_ids, gt_boxes, gt_masks = load_image_gt(
                    dataset,
                    self.config,
                    image_id,
                    augment=self.augment,
                    augmentation=self.augmentation,
                    use_mini_mask=self.config.USE_MINI_MASK,
                    epoch=self.epoch,
                )

            #             print(image_meta.shape)

            # Skip images that have no instances. This can happen in cases
            # where we train on a subset of classes and the image doesn't
            # have any of the classes we care about.
            if not np.any(gt_class_ids > 0):
                continue

            # RPN Targets
            rpn_match, rpn_bbox, rpn_match_class = build_rpn_targets(
                image.shape, self.anchors, gt_class_ids, gt_boxes, self.config
            )
            #             rpn_match, rpn_bbox ,anchors= build_rpn_targets(image.shape, anchors,
            #                                                     gt_class_ids, gt_boxes, config) # added by Marzi
            # Mask R-CNN Targets
            if self.random_rois:
                rpn_rois = generate_random_rois(
                    image.shape, self.random_rois, gt_class_ids, gt_boxes
                )
                if detection_targets:
                    (
                        rois,
                        mrcnn_class_ids,
                        mrcnn_bbox,
                        mrcnn_mask,
                    ) = build_detection_targets(
                        rpn_rois, gt_class_ids, gt_boxes, gt_masks, self.config
                    )

            # Init batch arrays
            if b == 0:
                batch_image_meta = np.zeros(
                    (batch_size,) + image_meta.shape, dtype=image_meta.dtype
                )
                batch_rpn_match = np.zeros(
                    [batch_size, self.anchors.shape[0], 1], dtype=rpn_match.dtype
                )
                batch_rpn_bbox = np.zeros(
                    [batch_size, self.config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4],
                    dtype=rpn_bbox.dtype,
                )
                batch_images = np.zeros((batch_size,) + image.shape, dtype=np.float32)
                batch_gt_class_ids = np.zeros(
                    (batch_size, self.config.MAX_GT_INSTANCES), dtype=np.int32
                )
                batch_gt_boxes = np.zeros(
                    (batch_size, self.config.MAX_GT_INSTANCES, 4), dtype=np.int32
                )
                batch_gt_masks = np.zeros(
                    (
                        batch_size,
                        gt_masks.shape[0],
                        gt_masks.shape[1],
                        self.config.MAX_GT_INSTANCES,
                    ),
                    dtype=gt_masks.dtype,
                )
                if self.random_rois:
                    batch_rpn_rois = np.zeros(
                        (batch_size, rpn_rois.shape[0], 4), dtype=rpn_rois.dtype
                    )
                    if detection_targets:
                        batch_rois = np.zeros(
                            (batch_size,) + rois.shape, dtype=rois.dtype
                        )
                        batch_mrcnn_class_ids = np.zeros(
                            (batch_size,) + mrcnn_class_ids.shape,
                            dtype=mrcnn_class_ids.dtype,
                        )
                        batch_mrcnn_bbox = np.zeros(
                            (batch_size,) + mrcnn_bbox.shape, dtype=mrcnn_bbox.dtype
                        )
                        batch_mrcnn_mask = np.zeros(
                            (batch_size,) + mrcnn_mask.shape, dtype=mrcnn_mask.dtype
                        )

            # If more instances than fits in the array, sub-sample from them.
            if gt_boxes.shape[0] > self.config.MAX_GT_INSTANCES:
                #                 ids = np.random.choice(
                #                     np.arange(gt_boxes.shape[0]), config.MAX_GT_INSTANCES, replace=False)
                #                 gt_class_ids = gt_class_ids[ids]
                gt_class_ids = gt_class_ids[0 : self.config.MAX_GT_INSTANCES]
                gt_boxes = gt_boxes[0 : self.config.MAX_GT_INSTANCES]
                gt_masks = gt_masks[:, :, 0 : self.config.MAX_GT_INSTANCES]

            # Add to batch
            batch_image_meta[b] = image_meta
            batch_rpn_match[b] = rpn_match[:, np.newaxis]
            batch_rpn_bbox[b] = rpn_bbox
            batch_images[b] = mold_image(image.astype(np.float32), self.config)
            batch_gt_class_ids[b, : gt_class_ids.shape[0]] = gt_class_ids
            batch_gt_boxes[b, : gt_boxes.shape[0]] = gt_boxes
            batch_gt_masks[b, :, :, : gt_masks.shape[-1]] = gt_masks
            if self.random_rois:
                batch_rpn_rois[b] = rpn_rois
                if detection_targets:
                    batch_rois[b] = rois
                    batch_mrcnn_class_ids[b] = mrcnn_class_ids
                    batch_mrcnn_bbox[b] = mrcnn_bbox
                    batch_mrcnn_mask[b] = mrcnn_mask
            b += 1

        # Batch full?
        #         if b >= batch_size:
        inputs = [batch_images, batch_image_meta, batch_rpn_bbox]
        #                 print("batch_gt_boxes",batch_gt_boxes)
        outputs = []

        if self.random_rois:
            inputs.extend([batch_rpn_rois])
            if detection_targets:
                inputs.extend([batch_rois])
                # Keras requires that output and targets have the same number of dimensions
                batch_mrcnn_class_ids = np.expand_dims(batch_mrcnn_class_ids, -1)
                outputs.extend(
                    [batch_mrcnn_class_ids, batch_mrcnn_bbox, batch_mrcnn_mask]
                )

        return inputs, outputs

    def on_epoch_end(self):
        """Updates indexes after each epoch"""
        #         self.indexes = np.arange(len(self.list_IDs))
        self.site_step += 1
        self.epoch += 1

        self.indexes = np.arange(
            0,
            len(self.dataset_inp[self.site_step % self.n_total_sites].image_ids),
            self.batch_size,
        )


# -----------------------------------------------------------------------------------------
class DataGeneratorBatch(KU.Sequence):
    """This class reads presaved data for all 9 cycles and max project them to calculate locations"""

    def __init__(
        self,
        dataset_inp,
        config,
        shuffle=False,
        augment=False,
        augmentation=None,
        random_rois=0,
        batch_size=1,
        detection_targets=False,
        no_augmentation_sources=None,
    ):

        """Initialization
        dataset: The Dataset object to pick data from
        config: The model config object
        shuffle: If True, shuffles the samples before every epoch
        augment: (deprecated. Use augmentation instead). If true, apply random
            image augmentation. Currently, only horizontal flipping is offered.
        augmentation: Optional. An imgaug (https://github.com/aleju/imgaug) augmentation.
            For example, passing imgaug.augmenters.Fliplr(0.5) flips images
            right/left 50% of the time.
        random_rois: If > 0 then generate proposals to be used to train the
                     network classifier and mask heads. Useful if training
                     the Mask RCNN part without the RPN.
        batch_size: How many images to return in each call
        detection_targets: If True, generate detection targets (class IDs, bbox
            deltas, and masks). Typically for debugging or visualizations because
            in trainig detection targets are generated by DetectionTargetLayer.
        no_augmentation_sources: Optional. List of sources to exclude for
            augmentation. A source is string that identifies a dataset and is
            defined in the Dataset class.
        """
        self.epoch = -1
        self.site_step = -1
        self.dataset_inp = dataset_inp
        self.n_total_sites = len(dataset_inp)
        self.config = config
        self.augment = augment
        self.augmentation = augmentation
        self.random_rois = random_rois
        self.batch_size = batch_size
        self.detection_targets = detection_targets
        self.no_augmentation_sources = no_augmentation_sources or []

        self.indexes = np.arange(0, len(dataset_inp[0].image_ids), self.batch_size)
        # Anchors
        # [anchor_count, (y1, x1, y2, x2)]
        self.backbone_shapes = compute_backbone_shapes(
            self.config, self.config.IMAGE_SHAPE
        )
        self.anchors = utils.generate_pyramid_anchors(
            self.config.RPN_ANCHOR_SCALES,
            self.config.RPN_ANCHOR_RATIOS,
            self.backbone_shapes,
            self.config.BACKBONE_STRIDES,
            self.config.RPN_ANCHOR_STRIDE,
        )

        self.on_epoch_end()

    def __len__(self):
        """Denotes the number of batches per epoch
        :return: number of batches per epoch
        """
        return self.config.STEPS_PER_EPOCH

    def __getitem__(self, index):
        """Generate one batch of data

        index: index of the batch
        return: X and y when fitting. X only when predicting
        """
        # Generate indexes of the batch by checking the current site first

        #         dataset=random.choice(dataset_inp); # for when we want to randomly select a site

        #         When site_step or epoch exceeds number of sites in the list of dataset_inp, we again go through
        #         the list from start
        dataset = self.dataset_inp[self.site_step % self.n_total_sites]

        #         print('epoch: ', self.epoch)
        #         print('Site: ', int(dataset.image_info[0]["site"]))

        # image ids for each site which are basically ids for crops
        image_ids = np.copy(dataset.image_ids)

        #         print('image_ids:',image_ids)
        #         print('image_index:',image_index)

        #         print('ind:',index,index*9,(index*9) % len(image_ids))
        #         image_index_firstSeq = (self.indexes[index]) % len(image_ids)

        #         When index exceeds number of cropped sequences in each site, we again go through
        #         the crops from the start
        index = index % len(self.indexes)

        image_index_firstSeq = self.indexes[index]
        #         print('image_index_firstSeq: ', index,image_index_firstSeq)

        #         get the list of image_ids for a cropped sequence
        batch_image_ids = image_ids[
            image_index_firstSeq : image_index_firstSeq + self.batch_size
        ]

        inputs, outputs = self.__data_generation(dataset, batch_image_ids)

        while inputs == []:
            print("jumped over ", image_index_firstSeq)
            image_index_firstSeq += self.batch_size
            batch_image_ids = image_ids[
                image_index_firstSeq : image_index_firstSeq + self.batch_size
            ]
            inputs, outputs = self.__data_generation(dataset, batch_image_ids)

        return inputs, outputs

    def __data_generation(self, dataset, batch_image_ids):
        "Generates data containing batch_size samples"  # X : (n_samples, *dim, n_channels)
        # Initialization

        batch_size = self.batch_size
        img_site = int(dataset.image_info[0]["site"])
        images = dataset.load_batch_ims_cr_presaved(
            batch_image_ids[0], self.config.batchplate_well
        )
        gt_masks, class_ids_batch = dataset.create_mask_batch(images)

        if gt_masks == []:
            return [], []

        _idx = np.sum(gt_masks, axis=(0, 1)) > 0
        gt_masks = gt_masks[:, :, _idx]
        class_ids_batch = class_ids_batch[_idx, :]

        gt_boxes = utils.extract_bboxes(gt_masks)

        active_class_ids = np.zeros([5], dtype=np.int32)
        source_class_ids = [0, 1, 2, 3, 4]
        active_class_ids[source_class_ids] = 1
        original_shape = (256, 256, 4)
        image_shape = (256, 256, 4)
        window = [0, 0, 256, 256]
        scale = 1

        if self.config.USE_MINI_MASK:
            gt_masks = utils.minimize_mask(
                gt_boxes, gt_masks, self.config.MINI_MASK_SHAPE
            )

        b = 0
        for image_id in batch_image_ids:
            image = images[:, :, :, b]
            gt_class_ids = class_ids_batch[:, b]
            image_meta = compose_image_meta(
                image_id,
                original_shape,
                image_shape,
                window,
                scale,
                active_class_ids,
                [img_site, self.epoch],
            )

            if not np.any(gt_class_ids > 0):
                continue

            # RPN Targets
            rpn_match, rpn_bbox, rpn_match_class = build_rpn_targets(
                image.shape, self.anchors, gt_class_ids, gt_boxes, self.config
            )

            #             print("rpn_match",rpn_match.shape)
            #             rpn_match, rpn_bbox ,anchors= build_rpn_targets(image.shape, anchors,
            #                                                     gt_class_ids, gt_boxes, config) # added by Marzi
            # Mask R-CNN Targets
            if self.random_rois:
                rpn_rois = generate_random_rois(
                    image.shape, self.random_rois, gt_class_ids, gt_boxes
                )
                if detection_targets:
                    (
                        rois,
                        mrcnn_class_ids,
                        mrcnn_bbox,
                        mrcnn_mask,
                    ) = build_detection_targets(
                        rpn_rois, gt_class_ids, gt_boxes, gt_masks, self.config
                    )

            # Init batch arrays
            if b == 0:
                batch_image_meta = np.zeros(
                    (batch_size,) + image_meta.shape, dtype=image_meta.dtype
                )
                batch_rpn_match = np.zeros(
                    [batch_size, self.anchors.shape[0], 1], dtype=rpn_match.dtype
                )
                batch_rpn_bbox = np.zeros(
                    [batch_size, self.config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4],
                    dtype=rpn_bbox.dtype,
                )
                batch_images = np.zeros((batch_size,) + image.shape, dtype=np.float32)
                batch_gt_class_ids = np.zeros(
                    (batch_size, self.config.MAX_GT_INSTANCES), dtype=np.int32
                )
                batch_gt_boxes = np.zeros(
                    (batch_size, self.config.MAX_GT_INSTANCES, 4), dtype=np.int32
                )
                batch_gt_masks = np.zeros(
                    (
                        batch_size,
                        gt_masks.shape[0],
                        gt_masks.shape[1],
                        self.config.MAX_GT_INSTANCES,
                    ),
                    dtype=gt_masks.dtype,
                )
                if self.random_rois:
                    batch_rpn_rois = np.zeros(
                        (batch_size, rpn_rois.shape[0], 4), dtype=rpn_rois.dtype
                    )
                    if detection_targets:
                        batch_rois = np.zeros(
                            (batch_size,) + rois.shape, dtype=rois.dtype
                        )
                        batch_mrcnn_class_ids = np.zeros(
                            (batch_size,) + mrcnn_class_ids.shape,
                            dtype=mrcnn_class_ids.dtype,
                        )
                        batch_mrcnn_bbox = np.zeros(
                            (batch_size,) + mrcnn_bbox.shape, dtype=mrcnn_bbox.dtype
                        )
                        batch_mrcnn_mask = np.zeros(
                            (batch_size,) + mrcnn_mask.shape, dtype=mrcnn_mask.dtype
                        )

            # If more instances than fits in the array, sub-sample from them.
            #             if gt_boxes.shape[0] > self.config.MAX_GT_INSTANCES:
            if gt_class_ids.shape[0] > self.config.MAX_GT_INSTANCES:
                #                 ids = np.random.choice(
                #                     np.arange(gt_boxes.shape[0]), config.MAX_GT_INSTANCES, replace=False)
                #                 gt_class_ids = gt_class_ids[ids]
                gt_class_ids = gt_class_ids[0 : self.config.MAX_GT_INSTANCES]
                gt_boxes = gt_boxes[0 : self.config.MAX_GT_INSTANCES]
                gt_masks = gt_masks[:, :, 0 : self.config.MAX_GT_INSTANCES]

            # Add to batch
            batch_image_meta[b] = image_meta
            batch_rpn_match[b] = rpn_match[:, np.newaxis]
            batch_rpn_bbox[b] = rpn_bbox
            batch_images[b] = mold_image(image.astype(np.float32), self.config)
            batch_gt_class_ids[b, : gt_class_ids.shape[0]] = gt_class_ids
            batch_gt_boxes[b, : gt_boxes.shape[0]] = gt_boxes
            batch_gt_masks[b, :, :, : gt_masks.shape[-1]] = gt_masks
            if self.random_rois:
                batch_rpn_rois[b] = rpn_rois
                if detection_targets:
                    batch_rois[b] = rois
                    batch_mrcnn_class_ids[b] = mrcnn_class_ids
                    batch_mrcnn_bbox[b] = mrcnn_bbox
                    batch_mrcnn_mask[b] = mrcnn_mask
            b += 1

        # Batch full?
        #         if b >= batch_size:
        inputs = [
            batch_images,
            batch_image_meta,
            batch_rpn_match,
            batch_rpn_bbox,
            batch_gt_class_ids,
            batch_gt_boxes,
            batch_gt_masks,
        ]
        #                 print("batch_gt_boxes",batch_gt_boxes)
        outputs = []

        if self.random_rois:
            inputs.extend([batch_rpn_rois])
            if detection_targets:
                inputs.extend([batch_rois])
                # Keras requires that output and targets have the same number of dimensions
                batch_mrcnn_class_ids = np.expand_dims(batch_mrcnn_class_ids, -1)
                outputs.extend(
                    [batch_mrcnn_class_ids, batch_mrcnn_bbox, batch_mrcnn_mask]
                )

        return inputs, outputs

    def on_epoch_end(self):
        """Updates indexes after each epoch"""
        #         self.indexes = np.arange(len(self.list_IDs))
        self.site_step += 1
        self.epoch += 1

        self.indexes = np.arange(
            0,
            len(self.dataset_inp[self.site_step % self.n_total_sites].image_ids),
            self.batch_size,
        )


# -----------------------------------------------------------------------------------------


def data_generator(
    dataset_inp,
    config,
    shuffle=False,
    augment=False,
    augmentation=None,
    random_rois=0,
    batch_size=1,
    detection_targets=False,
    no_augmentation_sources=None,
):
    """A generator that returns images and corresponding target class ids,
    bounding box deltas, and masks.

    dataset: The Dataset object to pick data from
    config: The model config object
    shuffle: If True, shuffles the samples before every epoch
    augment: (deprecated. Use augmentation instead). If true, apply random
        image augmentation. Currently, only horizontal flipping is offered.
    augmentation: Optional. An imgaug (https://github.com/aleju/imgaug) augmentation.
        For example, passing imgaug.augmenters.Fliplr(0.5) flips images
        right/left 50% of the time.
    random_rois: If > 0 then generate proposals to be used to train the
                 network classifier and mask heads. Useful if training
                 the Mask RCNN part without the RPN.
    batch_size: How many images to return in each call
    detection_targets: If True, generate detection targets (class IDs, bbox
        deltas, and masks). Typically for debugging or visualizations because
        in trainig detection targets are generated by DetectionTargetLayer.
    no_augmentation_sources: Optional. List of sources to exclude for
        augmentation. A source is string that identifies a dataset and is
        defined in the Dataset class.

    Returns a Python generator. Upon calling next() on it, the
    generator returns two lists, inputs and outputs. The contents
    of the lists differs depending on the received arguments:
    inputs list:
    - images: [batch, H, W, C]
    - image_meta: [batch, (meta data)] Image details. See compose_image_meta()
    - rpn_match: [batch, N] Integer (1=positive anchor, -1=negative, 0=neutral)
    - rpn_bbox: [batch, N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.
    - gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs
    - gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)]
    - gt_masks: [batch, height, width, MAX_GT_INSTANCES]. The height and width
                are those of the image unless use_mini_mask is True, in which
                case they are defined in MINI_MASK_SHAPE.

    outputs list: Usually empty in regular training. But if detection_targets
        is True then the outputs list contains target class_ids, bbox deltas,
        and masks.
    """
    #     epoch_number = 1
    b = 0  # batch item index
    image_index = -1
    #     image_ids = np.copy(dataset.image_ids)
    error_count = 0
    no_augmentation_sources = no_augmentation_sources or []

    # Anchors
    # [anchor_count, (y1, x1, y2, x2)]
    backbone_shapes = compute_backbone_shapes(config, config.IMAGE_SHAPE)
    anchors = utils.generate_pyramid_anchors(
        config.RPN_ANCHOR_SCALES,
        config.RPN_ANCHOR_RATIOS,
        backbone_shapes,
        config.BACKBONE_STRIDES,
        config.RPN_ANCHOR_STRIDE,
    )

    if config.DATA_GEN == "well":
        #         n_sites=len(dataset_inp);
        #         np.random.choice(list(range(n_sites))
        dataset = random.choice(dataset_inp)
        print("Site which is sampled: ", int(dataset.image_info[0]["site"]))
    #         print('Site which is sampled: ', int(dataset.image_info[10]["overlay_dir"].split('_')[-2]))
    else:
        dataset = dataset_inp

    image_ids = np.copy(dataset.image_ids)
    print("image_ids:", image_ids)
    print("image_index:", image_index)

    # Keras requires a generator to run indefinitely.
    while True:
        try:
            # Increment index to pick next image. Shuffle if at the start of an epoch.
            #             print('image_index',image_index,' image_ids',image_ids)
            image_index = (image_index + 1) % len(image_ids)
            #             print('image_index2:',image_index)
            #             print('b:',b)
            #             if shuffle and image_index == 0:
            #                 np.random.shuffle(image_ids)

            #             # Get GT bounding boxes and masks for image.
            #             if image_index == 0:
            #                 image_index=np.random.choice(range(0,len(image_ids),9))

            image_id = image_ids[image_index]

            #             print('image_id',image_id)
            # If the image source is not to be augmented pass None as augmentation
            if dataset.image_info[image_id]["source"] in no_augmentation_sources:
                image, image_meta, gt_class_ids, gt_boxes, gt_masks = load_image_gt(
                    dataset,
                    config,
                    image_id,
                    augment=augment,
                    augmentation=None,
                    use_mini_mask=config.USE_MINI_MASK,
                )

            else:
                image, image_meta, gt_class_ids, gt_boxes, gt_masks = load_image_gt(
                    dataset,
                    config,
                    image_id,
                    augment=augment,
                    augmentation=augmentation,
                    use_mini_mask=config.USE_MINI_MASK,
                )

            #             print(image_meta.shape)

            # Skip images that have no instances. This can happen in cases
            # where we train on a subset of classes and the image doesn't
            # have any of the classes we care about.
            if not np.any(gt_class_ids > 0):
                continue

            # RPN Targets
            rpn_match, rpn_bbox, rpn_match_class = build_rpn_targets(
                image.shape, anchors, gt_class_ids, gt_boxes, config
            )
            #             rpn_match, rpn_bbox ,anchors= build_rpn_targets(image.shape, anchors,
            #                                                     gt_class_ids, gt_boxes, config) # added by Marzi
            # Mask R-CNN Targets
            if random_rois:
                rpn_rois = generate_random_rois(
                    image.shape, random_rois, gt_class_ids, gt_boxes
                )
                if detection_targets:
                    (
                        rois,
                        mrcnn_class_ids,
                        mrcnn_bbox,
                        mrcnn_mask,
                    ) = build_detection_targets(
                        rpn_rois, gt_class_ids, gt_boxes, gt_masks, config
                    )

            # Init batch arrays
            if b == 0:
                batch_image_meta = np.zeros(
                    (batch_size,) + image_meta.shape, dtype=image_meta.dtype
                )
                batch_rpn_match = np.zeros(
                    [batch_size, anchors.shape[0], 1], dtype=rpn_match.dtype
                )
                batch_rpn_bbox = np.zeros(
                    [batch_size, config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4],
                    dtype=rpn_bbox.dtype,
                )
                batch_images = np.zeros((batch_size,) + image.shape, dtype=np.float32)
                batch_gt_class_ids = np.zeros(
                    (batch_size, config.MAX_GT_INSTANCES), dtype=np.int32
                )
                batch_gt_boxes = np.zeros(
                    (batch_size, config.MAX_GT_INSTANCES, 4), dtype=np.int32
                )
                batch_gt_masks = np.zeros(
                    (
                        batch_size,
                        gt_masks.shape[0],
                        gt_masks.shape[1],
                        config.MAX_GT_INSTANCES,
                    ),
                    dtype=gt_masks.dtype,
                )
                if random_rois:
                    batch_rpn_rois = np.zeros(
                        (batch_size, rpn_rois.shape[0], 4), dtype=rpn_rois.dtype
                    )
                    if detection_targets:
                        batch_rois = np.zeros(
                            (batch_size,) + rois.shape, dtype=rois.dtype
                        )
                        batch_mrcnn_class_ids = np.zeros(
                            (batch_size,) + mrcnn_class_ids.shape,
                            dtype=mrcnn_class_ids.dtype,
                        )
                        batch_mrcnn_bbox = np.zeros(
                            (batch_size,) + mrcnn_bbox.shape, dtype=mrcnn_bbox.dtype
                        )
                        batch_mrcnn_mask = np.zeros(
                            (batch_size,) + mrcnn_mask.shape, dtype=mrcnn_mask.dtype
                        )

            # If more instances than fits in the array, sub-sample from them.
            if gt_boxes.shape[0] > config.MAX_GT_INSTANCES:
                #                 ids = np.random.choice(
                #                     np.arange(gt_boxes.shape[0]), config.MAX_GT_INSTANCES, replace=False)
                #                 gt_class_ids = gt_class_ids[ids]
                gt_class_ids = gt_class_ids[0 : config.MAX_GT_INSTANCES]
                gt_boxes = gt_boxes[0 : config.MAX_GT_INSTANCES]
                gt_masks = gt_masks[:, :, 0 : config.MAX_GT_INSTANCES]

            # Add to batch
            batch_image_meta[b] = image_meta
            batch_rpn_match[b] = rpn_match[:, np.newaxis]
            batch_rpn_bbox[b] = rpn_bbox
            batch_images[b] = mold_image(image.astype(np.float32), config)
            batch_gt_class_ids[b, : gt_class_ids.shape[0]] = gt_class_ids
            batch_gt_boxes[b, : gt_boxes.shape[0]] = gt_boxes
            batch_gt_masks[b, :, :, : gt_masks.shape[-1]] = gt_masks
            if random_rois:
                batch_rpn_rois[b] = rpn_rois
                if detection_targets:
                    batch_rois[b] = rois
                    batch_mrcnn_class_ids[b] = mrcnn_class_ids
                    batch_mrcnn_bbox[b] = mrcnn_bbox
                    batch_mrcnn_mask[b] = mrcnn_mask
            b += 1

            # Batch full?
            if b >= batch_size:
                inputs = [
                    batch_images,
                    batch_image_meta,
                    batch_rpn_match,
                    batch_rpn_bbox,
                    batch_gt_class_ids,
                    batch_gt_boxes,
                    batch_gt_masks,
                ]
                #                 print("batch_gt_boxes",batch_gt_boxes)
                outputs = []

                if random_rois:
                    inputs.extend([batch_rpn_rois])
                    if detection_targets:
                        inputs.extend([batch_rois])
                        # Keras requires that output and targets have the same number of dimensions
                        batch_mrcnn_class_ids = np.expand_dims(
                            batch_mrcnn_class_ids, -1
                        )
                        outputs.extend(
                            [batch_mrcnn_class_ids, batch_mrcnn_bbox, batch_mrcnn_mask]
                        )

                yield inputs, outputs

                # start a new batch
                b = 0
        except (GeneratorExit, KeyboardInterrupt):
            raise
        except:
            # Log it and skip the image
            logging.exception(
                "Error processing image {}".format(dataset.image_info[image_id])
            )
            error_count += 1
            if error_count > 5:
                raise


def data_generator_test(
    dataset_inp,
    config,
    shuffle=False,
    augment=False,
    augmentation=None,
    random_rois=0,
    batch_size=1,
    detection_targets=False,
    no_augmentation_sources=None,
    anchors=[],
):
    """A generator"""
    #     epoch_number = 1
    #     b = 0  # batch item index
    #     image_index = -1
    #     image_ids = np.copy(dataset.image_ids)
    error_count = 0
    no_augmentation_sources = no_augmentation_sources or []

    # Anchors
    # [anchor_count, (y1, x1, y2, x2)]
    backbone_shapes = compute_backbone_shapes(config, config.IMAGE_SHAPE)
    if anchors == []:
        anchors = utils.generate_pyramid_anchors(
            config.RPN_ANCHOR_SCALES,
            config.RPN_ANCHOR_RATIOS,
            backbone_shapes,
            config.BACKBONE_STRIDES,
            config.RPN_ANCHOR_STRIDE,
        )

    n_sites = len(dataset_inp)
    site_ind = 0

    for si in range(n_sites):
        print("site:", si)
        #         try:
        dataset = dataset_inp[si]
        image_ids = np.copy(dataset.image_ids)
        print("image_ids:", image_ids)
        #         print('image_index:',image_index)

        batch_start_indices = np.arange(0, len(image_ids), batch_size)

        #         image_index = -1

        #         if config.DATA_GEN=='well':
        #     #         n_sites=len(dataset_inp);
        #     #         np.random.choice(list(range(n_sites))
        #             dataset=random.choice(dataset_inp);
        #             print('Site which is sampled: ', int(dataset.image_info[0]["site"]))
        #     #         print('Site which is sampled: ', int(dataset.image_info[10]["overlay_dir"].split('_')[-2]))
        #         else:
        #             dataset=dataset_inp;

        # Keras requires a generator to run indefinitely.
        #         while True:
        for bsi in batch_start_indices:

            #             b = 0  # batch item index
            image_index = bsi

            try:
                for b in range(batch_size):
                    # Increment index to pick next image. Shuffle if at the start of an epoch.
                    #             print('image_index',image_index,' image_ids',image_ids)
                    #                 image_index = (image_index + 1) % len(image_ids)
                    image_index = image_index + 1
                    #             print('image_index2:',image_index)

                    image_id = image_ids[image_index]

                    # If the image source is not to be augmented pass None as augmentation
                    image, image_meta, gt_class_ids, gt_boxes, gt_masks = load_image_gt(
                        dataset,
                        config,
                        image_id,
                        augment=augment,
                        augmentation=None,
                        use_mini_mask=config.USE_MINI_MASK,
                    )

                    # Skip images that have no instances. This can happen in cases
                    # where we train on a subset of classes and the image doesn't
                    # have any of the classes we care about.
                    if not np.any(gt_class_ids > 0):
                        continue

                    # RPN Targets
                    #                     rpn_bbox=anchors
                    #                     rpn_match, rpn_bbox = build_rpn_targets(image.shape, anchors,
                    #                                                             gt_class_ids, gt_boxes, config)
                    #             rpn_match, rpn_bbox ,anchors= build_rpn_targets(image.shape, anchors,
                    #                                                     gt_class_ids, gt_boxes, config) # added by Marzi
                    # Mask R-CNN Targets
                    if random_rois:
                        rpn_rois = generate_random_rois(
                            image.shape, random_rois, gt_class_ids, gt_boxes
                        )
                        if detection_targets:
                            (
                                rois,
                                mrcnn_class_ids,
                                mrcnn_bbox,
                                mrcnn_mask,
                            ) = build_detection_targets(
                                rpn_rois, gt_class_ids, gt_boxes, gt_masks, config
                            )

                    # Init batch arrays
                    if b == 0:
                        batch_image_meta = np.zeros(
                            (batch_size,) + image_meta.shape, dtype=image_meta.dtype
                        )
                        #                         batch_rpn_bbox = np.zeros(
                        #                             [batch_size, config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4], dtype=rpn_bbox.dtype)
                        batch_images = np.zeros(
                            (batch_size,) + image.shape, dtype=np.float32
                        )

                    # Add to batch
                    batch_image_meta[b] = image_meta
                    #                     batch_rpn_match[b] = rpn_match[:, np.newaxis]
                    #                     batch_rpn_bbox[b] = rpn_bbox
                    batch_images[b] = mold_image(image.astype(np.float32), config)
                    #                     batch_gt_class_ids[b, :gt_class_ids.shape[0]] = gt_class_ids
                    #                     batch_gt_boxes[b, :gt_boxes.shape[0]] = gt_boxes
                    #                     batch_gt_masks[b, :, :, :gt_masks.shape[-1]] = gt_masks
                    if random_rois:
                        batch_rpn_rois[b] = rpn_rois
                        if detection_targets:
                            batch_rois[b] = rois
                            batch_mrcnn_class_ids[b] = mrcnn_class_ids
                            batch_mrcnn_bbox[b] = mrcnn_bbox
                            batch_mrcnn_mask[b] = mrcnn_mask
                #                     b += 1

                # Batch full?
                #                     if b >= batch_size:

                #                 inputs = [batch_images, batch_image_meta, batch_rpn_match, batch_rpn_bbox,
                #                           batch_gt_class_ids, batch_gt_boxes, batch_gt_masks]
                inputs = [batch_images, batch_image_meta, anchors]
                #                 print("batch_gt_boxes",batch_gt_boxes)
                outputs = []

                if random_rois:
                    inputs.extend([batch_rpn_rois])
                    if detection_targets:
                        inputs.extend([batch_rois])
                        # Keras requires that output and targets have the same number of dimensions
                        batch_mrcnn_class_ids = np.expand_dims(
                            batch_mrcnn_class_ids, -1
                        )
                        outputs.extend(
                            [batch_mrcnn_class_ids, batch_mrcnn_bbox, batch_mrcnn_mask]
                        )

                yield inputs, outputs

                # start a new batch
            #                         b = 0
            except (GeneratorExit, KeyboardInterrupt):
                raise
            except:
                # Log it and skip the image
                logging.exception(
                    "Error processing image {}".format(dataset.image_info[image_id])
                )
                error_count += 1
                if error_count > 5:
                    raise


#         except (GeneratorExit, KeyboardInterrupt):
#             raise


############################################################
#  Data Generator
############################################################


def load_image_gt(
    dataset,
    config,
    image_id,
    augment=False,
    augmentation=None,
    use_mini_mask=False,
    epoch=0,
):
    """Load and return ground truth data for an image (image, mask, bounding boxes).

    augment: (deprecated. Use augmentation instead). If true, apply random
        image augmentation. Currently, only horizontal flipping is offered.
    augmentation: Optional. An imgaug (https://github.com/aleju/imgaug) augmentation.
        For example, passing imgaug.augmenters.Fliplr(0.5) flips images
        right/left 50% of the time.
    use_mini_mask: If False, returns full-size masks that are the same height
        and width as the original image. These can be big, for example
        1024x1024x100 (for 100 instances). Mini masks are smaller, typically,
        224x224 and are generated by extracting the bounding box of the
        object and resizing it to MINI_MASK_SHAPE.

    Returns:
    image: [height, width, 3]
    shape: the original shape of the image before resizing and cropping.
    class_ids: [instance_count] Integer class IDs
    bbox: [instance_count, (y1, x1, y2, x2)]
    mask: [height, width, instance_count]. The height and width are those
        of the image unless use_mini_mask is True, in which case they are
        defined in MINI_MASK_SHAPE.

    """

    # Load image and mask
    #     image = dataset.load_image(image_id)

    if config.load_cropped_presaved:
        image = dataset.load_image_cr_presaved(image_id, config.batchplate_well)
    else:
        image = dataset.load_image(image_id)

    #     pdb.set_trace()

    if config.create_mask:
        #         mask, class_ids = dataset.create_mask(image_id,image)
        #         mask, class_ids = dataset.create_mask(image)
        #         bbox = utils.extract_bboxes(mask)
        bbox, class_ids = dataset.create_bbox(image)

    #         print("bbox",bbox.shape,bbox[:5,:],class_ids[:5])
    #         print("bbox2",bbox2.shape,bbox2[:5,:],class_ids2[:5])

    #         pdb.set_trace()

    else:
        #         mask, class_ids = dataset.load_mask(image_id)
        bbox, class_ids = dataset.load_bbox(image_id)
    #         mask, class_ids = dataset.load_mask_cr_presaved(image_id)
    #     print('m1',mask.shape)
    #     print('i1',image.shape)

    original_shape = image.shape
    image, window, scale, padding, crop = utils.resize_image(
        image,
        min_dim=config.IMAGE_MIN_DIM,
        min_scale=config.IMAGE_MIN_SCALE,
        max_dim=config.IMAGE_MAX_DIM,
        mode=config.IMAGE_RESIZE_MODE,
    )

    if (config.IMAGE_MIN_DIM != original_shape[0]) or (
        config.IMAGE_MAX_DIM != original_shape[0]
    ):
        raise Exception(
            "Input Images should be square and resizing is not implemented, to do this make sure you are resizing bbox variable!"
        )

    #     mask = utils.resize_mask(mask, scale, padding, crop)

    #     # Note that some boxes might be all zeros if the corresponding mask got cropped out.
    #     # and here is to filter them out
    #     _idx = np.sum(mask, axis=(0, 1)) > 0
    #     mask = mask[:, :, _idx]
    #     class_ids = class_ids[_idx]
    #     # Bounding boxes. Note that some boxes might be all zeros
    #     # if the corresponding mask got cropped out.
    #     # bbox: [num_instances, (y1, x1, y2, x2)]
    #     bbox = utils.extract_bboxes(mask)

    # Active classes
    # Different datasets have different classes, so track the
    # classes supported in the dataset of this image.
    #     print("n_c",dataset.num_classes)

    active_class_ids = np.zeros(
        [dataset.num_classes], dtype=np.int32
    )  # commented by Marzi
    #     active_class_ids = np.zeros([config.NUM_CLASSES], dtype=np.int32)
    source_class_ids = dataset.source_class_ids[dataset.image_info[image_id]["source"]]
    active_class_ids[source_class_ids] = 1

    #     # Resize masks to smaller size to reduce memory usage
    #     if use_mini_mask:
    #         mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)

    #     img_site=int(dataset.image_info[image_id]["overlay_dir"].split('_')[-2])
    img_site = dataset.image_info[image_id]["site"]
    # Image meta data
    image_meta = compose_image_meta(
        image_id,
        original_shape,
        image.shape,
        window,
        scale,
        active_class_ids,
        [img_site, epoch],
    )

    #         image_meta = compose_image_meta(image_id, original_shape, image.shape,
    #                                     window, scale, active_class_ids)
    #     print("image_meta_2",image_meta.shape,image_meta)
    return image, image_meta, class_ids, bbox


def load_image_gt_with_mask(
    dataset,
    config,
    image_id,
    augment=False,
    augmentation=None,
    use_mini_mask=False,
    epoch=0,
):
    """Load and return ground truth data for an image (image, mask, bounding boxes).

    augment: (deprecated. Use augmentation instead). If true, apply random
        image augmentation. Currently, only horizontal flipping is offered.
    augmentation: Optional. An imgaug (https://github.com/aleju/imgaug) augmentation.
        For example, passing imgaug.augmenters.Fliplr(0.5) flips images
        right/left 50% of the time.
    use_mini_mask: If False, returns full-size masks that are the same height
        and width as the original image. These can be big, for example
        1024x1024x100 (for 100 instances). Mini masks are smaller, typically,
        224x224 and are generated by extracting the bounding box of the
        object and resizing it to MINI_MASK_SHAPE.

    Returns:
    image: [height, width, 3]
    shape: the original shape of the image before resizing and cropping.
    class_ids: [instance_count] Integer class IDs
    bbox: [instance_count, (y1, x1, y2, x2)]
    mask: [height, width, instance_count]. The height and width are those
        of the image unless use_mini_mask is True, in which case they are
        defined in MINI_MASK_SHAPE.
    """
    # Load image and mask
    #     image = dataset.load_image(image_id)

    if config.load_cropped_presaved:
        image = dataset.load_image_cr_presaved(image_id, config.batchplate_well)
    else:
        image = dataset.load_image(image_id)

    if config.create_mask:
        mask, class_ids = dataset.create_mask(image_id, image)
    else:
        mask, class_ids = dataset.load_mask(image_id)
    #         mask, class_ids = dataset.load_mask_cr_presaved(image_id)
    #     print('m1',mask.shape)
    #     print('i1',image.shape)

    original_shape = image.shape
    image, window, scale, padding, crop = utils.resize_image(
        image,
        min_dim=config.IMAGE_MIN_DIM,
        min_scale=config.IMAGE_MIN_SCALE,
        max_dim=config.IMAGE_MAX_DIM,
        mode=config.IMAGE_RESIZE_MODE,
    )

    mask = utils.resize_mask(mask, scale, padding, crop)
    #     print('m2',mask.shape)
    #     print('i2',image.shape)
    # Random horizontal flips.
    # TODO: will be removed in a future update in favor of augmentation
    if augment:
        logging.warning("'augment' is deprecated. Use 'augmentation' instead.")
        if random.randint(0, 1):
            image = np.fliplr(image)
            mask = np.fliplr(mask)

    # Augmentation
    # This requires the imgaug lib (https://github.com/aleju/imgaug)
    if augmentation:
        import imgaug

        # Augmenters that are safe to apply to masks
        # Some, such as Affine, have settings that make them unsafe, so always
        # test your augmentation on masks
        MASK_AUGMENTERS = [
            "Sequential",
            "SomeOf",
            "OneOf",
            "Sometimes",
            "Fliplr",
            "Flipud",
            "CropAndPad",
            "Affine",
            "PiecewiseAffine",
        ]

        def hook(images, augmenter, parents, default):
            """Determines which augmenters to apply to masks."""
            return augmenter.__class__.__name__ in MASK_AUGMENTERS

        # Store shapes before augmentation to compare
        image_shape = image.shape
        mask_shape = mask.shape
        # Make augmenters deterministic to apply similarly to images and masks
        det = augmentation.to_deterministic()
        image = det.augment_image(image)
        # Change mask to np.uint8 because imgaug doesn't support np.bool
        mask = det.augment_image(
            mask.astype(np.uint8), hooks=imgaug.HooksImages(activator=hook)
        )
        # Verify that shapes didn't change
        assert image.shape == image_shape, "Augmentation shouldn't change image size"
        assert mask.shape == mask_shape, "Augmentation shouldn't change mask size"
        # Change mask back to bool
        mask = mask.astype(np.bool)

    # Note that some boxes might be all zeros if the corresponding mask got cropped out.
    # and here is to filter them out
    _idx = np.sum(mask, axis=(0, 1)) > 0
    mask = mask[:, :, _idx]
    class_ids = class_ids[_idx]
    # Bounding boxes. Note that some boxes might be all zeros
    # if the corresponding mask got cropped out.
    # bbox: [num_instances, (y1, x1, y2, x2)]
    bbox = utils.extract_bboxes(mask)

    # Active classes
    # Different datasets have different classes, so track the
    # classes supported in the dataset of this image.
    #     print("n_c",dataset.num_classes)

    active_class_ids = np.zeros(
        [dataset.num_classes], dtype=np.int32
    )  # commented by Marzi
    #     active_class_ids = np.zeros([config.NUM_CLASSES], dtype=np.int32)
    source_class_ids = dataset.source_class_ids[dataset.image_info[image_id]["source"]]
    active_class_ids[source_class_ids] = 1

    # Resize masks to smaller size to reduce memory usage
    if use_mini_mask:
        mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)

    #     img_site=int(dataset.image_info[image_id]["overlay_dir"].split('_')[-2])
    img_site = dataset.image_info[image_id]["site"]
    # Image meta data
    image_meta = compose_image_meta(
        image_id,
        original_shape,
        image.shape,
        window,
        scale,
        active_class_ids,
        [img_site, epoch],
    )

    #         image_meta = compose_image_meta(image_id, original_shape, image.shape,
    #                                     window, scale, active_class_ids)
    #     print("image_meta_2",image_meta.shape,image_meta)
    return image, image_meta, class_ids, bbox, mask


def compute_backbone_shapes(config, image_shape):
    """Computes the width and height of each stage of the backbone network.

    Returns:
        [N, (height, width)]. Where N is the number of stages
    """
    if callable(config.BACKBONE):
        return config.COMPUTE_BACKBONE_SHAPE(image_shape)

    # Currently supports ResNet only
    assert config.BACKBONE in ["resnet18", "resnet50", "resnet101"]
    return np.array(
        [
            [
                int(math.ceil(image_shape[0] / stride)),
                int(math.ceil(image_shape[1] / stride)),
            ]
            for stride in config.BACKBONE_STRIDES
        ]
    )


def build_detection_targets(rpn_rois, gt_class_ids, gt_boxes, gt_masks, config):
    """Generate targets for training Stage 2 classifier and mask heads.
    This is not used in normal training. It's useful for debugging or to train
    the Mask RCNN heads without using the RPN head.

    Inputs:
    rpn_rois: [N, (y1, x1, y2, x2)] proposal boxes.
    gt_class_ids: [instance count] Integer class IDs
    gt_boxes: [instance count, (y1, x1, y2, x2)]
    gt_masks: [height, width, instance count] Ground truth masks. Can be full
              size or mini-masks.

    Returns:
    rois: [TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)]
    class_ids: [TRAIN_ROIS_PER_IMAGE]. Integer class IDs.
    bboxes: [TRAIN_ROIS_PER_IMAGE, NUM_CLASSES, (y, x, log(h), log(w))]. Class-specific
            bbox refinements.
    masks: [TRAIN_ROIS_PER_IMAGE, height, width, NUM_CLASSES). Class specific masks cropped
           to bbox boundaries and resized to neural network output size.
    """
    assert rpn_rois.shape[0] > 0
    assert gt_class_ids.dtype == np.int32, "Expected int but got {}".format(
        gt_class_ids.dtype
    )
    assert gt_boxes.dtype == np.int32, "Expected int but got {}".format(gt_boxes.dtype)
    assert gt_masks.dtype == np.bool_, "Expected bool but got {}".format(gt_masks.dtype)

    # It's common to add GT Boxes to ROIs but we don't do that here because
    # according to XinLei Chen's paper, it doesn't help.

    # Trim empty padding in gt_boxes and gt_masks parts
    instance_ids = np.where(gt_class_ids > 0)[0]
    assert instance_ids.shape[0] > 0, "Image must contain instances."
    gt_class_ids = gt_class_ids[instance_ids]
    gt_boxes = gt_boxes[instance_ids]
    gt_masks = gt_masks[:, :, instance_ids]

    # Compute areas of ROIs and ground truth boxes.
    rpn_roi_area = (rpn_rois[:, 2] - rpn_rois[:, 0]) * (rpn_rois[:, 3] - rpn_rois[:, 1])
    gt_box_area = (gt_boxes[:, 2] - gt_boxes[:, 0]) * (gt_boxes[:, 3] - gt_boxes[:, 1])

    # Compute overlaps [rpn_rois, gt_boxes]
    overlaps = np.zeros((rpn_rois.shape[0], gt_boxes.shape[0]))
    for i in range(overlaps.shape[1]):
        gt = gt_boxes[i]
        overlaps[:, i] = utils.compute_iou(gt, rpn_rois, gt_box_area[i], rpn_roi_area)

    # Assign ROIs to GT boxes
    rpn_roi_iou_argmax = np.argmax(overlaps, axis=1)
    rpn_roi_iou_max = overlaps[np.arange(overlaps.shape[0]), rpn_roi_iou_argmax]
    # GT box assigned to each ROI
    rpn_roi_gt_boxes = gt_boxes[rpn_roi_iou_argmax]
    rpn_roi_gt_class_ids = gt_class_ids[rpn_roi_iou_argmax]

    # Positive ROIs are those with >= 0.5 IoU with a GT box.
    fg_ids = np.where(rpn_roi_iou_max > 0.5)[0]

    # Negative ROIs are those with max IoU 0.1-0.5 (hard example mining)
    # TODO: To hard example mine or not to hard example mine, that's the question
    # bg_ids = np.where((rpn_roi_iou_max >= 0.1) & (rpn_roi_iou_max < 0.5))[0]
    bg_ids = np.where(rpn_roi_iou_max < 0.5)[0]

    # Subsample ROIs. Aim for 33% foreground.
    # FG
    fg_roi_count = int(config.TRAIN_ROIS_PER_IMAGE * config.ROI_POSITIVE_RATIO)
    if fg_ids.shape[0] > fg_roi_count:
        keep_fg_ids = np.random.choice(fg_ids, fg_roi_count, replace=False)
        #### balance random sampling of each class ID ### added by Marzi
    #         for

    else:
        keep_fg_ids = fg_ids
    # BG
    remaining = config.TRAIN_ROIS_PER_IMAGE - keep_fg_ids.shape[0]
    if bg_ids.shape[0] > remaining:
        keep_bg_ids = np.random.choice(bg_ids, remaining, replace=False)
    else:
        keep_bg_ids = bg_ids
    # Combine indices of ROIs to keep
    keep = np.concatenate([keep_fg_ids, keep_bg_ids])
    # Need more?
    remaining = config.TRAIN_ROIS_PER_IMAGE - keep.shape[0]
    if remaining > 0:
        # Looks like we don't have enough samples to maintain the desired
        # balance. Reduce requirements and fill in the rest. This is
        # likely different from the Mask RCNN paper.

        # There is a small chance we have neither fg nor bg samples.
        if keep.shape[0] == 0:
            # Pick bg regions with easier IoU threshold
            bg_ids = np.where(rpn_roi_iou_max < 0.5)[0]
            assert bg_ids.shape[0] >= remaining
            keep_bg_ids = np.random.choice(bg_ids, remaining, replace=False)
            assert keep_bg_ids.shape[0] == remaining
            keep = np.concatenate([keep, keep_bg_ids])
        else:
            # Fill the rest with repeated bg rois.
            keep_extra_ids = np.random.choice(keep_bg_ids, remaining, replace=True)
            keep = np.concatenate([keep, keep_extra_ids])
    assert (
        keep.shape[0] == config.TRAIN_ROIS_PER_IMAGE
    ), "keep doesn't match ROI batch size {}, {}".format(
        keep.shape[0], config.TRAIN_ROIS_PER_IMAGE
    )

    # Reset the gt boxes assigned to BG ROIs.
    rpn_roi_gt_boxes[keep_bg_ids, :] = 0
    rpn_roi_gt_class_ids[keep_bg_ids] = 0

    # For each kept ROI, assign a class_id, and for FG ROIs also add bbox refinement.
    rois = rpn_rois[keep]
    roi_gt_boxes = rpn_roi_gt_boxes[keep]
    roi_gt_class_ids = rpn_roi_gt_class_ids[keep]
    roi_gt_assignment = rpn_roi_iou_argmax[keep]

    # Class-aware bbox deltas. [y, x, log(h), log(w)]
    bboxes = np.zeros(
        (config.TRAIN_ROIS_PER_IMAGE, config.NUM_CLASSES, 4), dtype=np.float32
    )
    pos_ids = np.where(roi_gt_class_ids > 0)[0]
    bboxes[pos_ids, roi_gt_class_ids[pos_ids]] = utils.box_refinement(
        rois[pos_ids], roi_gt_boxes[pos_ids, :4]
    )
    # Normalize bbox refinements
    bboxes /= config.BBOX_STD_DEV

    # Generate class-specific target masks
    masks = np.zeros(
        (
            config.TRAIN_ROIS_PER_IMAGE,
            config.MASK_SHAPE[0],
            config.MASK_SHAPE[1],
            config.NUM_CLASSES,
        ),
        dtype=np.float32,
    )
    for i in pos_ids:
        class_id = roi_gt_class_ids[i]
        assert class_id > 0, "class id must be greater than 0"
        gt_id = roi_gt_assignment[i]
        class_mask = gt_masks[:, :, gt_id]

        if config.USE_MINI_MASK:
            # Create a mask placeholder, the size of the image
            placeholder = np.zeros(config.IMAGE_SHAPE[:2], dtype=bool)
            # GT box
            gt_y1, gt_x1, gt_y2, gt_x2 = gt_boxes[gt_id]
            gt_w = gt_x2 - gt_x1
            gt_h = gt_y2 - gt_y1
            # Resize mini mask to size of GT box
            placeholder[gt_y1:gt_y2, gt_x1:gt_x2] = np.round(
                utils.resize(class_mask, (gt_h, gt_w))
            ).astype(bool)
            # Place the mini batch in the placeholder
            class_mask = placeholder

        # Pick part of the mask and resize it
        y1, x1, y2, x2 = rois[i].astype(np.int32)
        m = class_mask[y1:y2, x1:x2]
        mask = utils.resize(m, config.MASK_SHAPE)
        masks[i, :, :, class_id] = mask

    return rois, roi_gt_class_ids, bboxes, masks


def build_rpn_targets_marzi(image_shape, anchors, gt_class_ids, gt_boxes, config):
    """Given the anchors and GT boxes, compute overlaps and identify positive
    anchors and deltas to refine them to match their corresponding GT boxes.
    anchors: [num_anchors, (y1, x1, y2, x2)]
    gt_class_ids: [num_gt_boxes] Integer class IDs.
    gt_boxes: [num_gt_boxes, (y1, x1, y2, x2)]
    Returns:
    rpn_match: [N] (int32) matches between anchors and GT boxes.
               1 = positive anchor, -1 = negative anchor, 0 = neutral
    rpn_bbox: [config.RPN_TRAIN_ANCHORS_PER_IMAGE, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.
    """
    # RPN Match: 1 = positive anchor, -1 = negative anchor, 0 = neutral
    rpn_match = np.zeros([anchors.shape[0]], dtype=np.int32)
    rpn_match_class = np.zeros([anchors.shape[0]], dtype=np.int32)
    # RPN bounding boxes: [max anchors per image, (dy, dx, log(dh), log(dw))]
    #     rpn_bbox = np.zeros((config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4))

    # Handle COCO crowds
    # A crowd box in COCO is a bounding box around several instances. Exclude
    # them from training. A crowd box is given a negative class ID.
    crowd_ix = np.where(gt_class_ids < 0)[0]
    if crowd_ix.shape[0] > 0:
        # Filter out crowds from ground truth class IDs and boxes
        non_crowd_ix = np.where(gt_class_ids > 0)[0]
        crowd_boxes = gt_boxes[crowd_ix]
        gt_class_ids = gt_class_ids[non_crowd_ix]
        gt_boxes = gt_boxes[non_crowd_ix]
        # Compute overlaps with crowd boxes [anchors, crowds]
        crowd_overlaps = utils.compute_overlaps(anchors, crowd_boxes)
        crowd_iou_max = np.amax(crowd_overlaps, axis=1)
        no_crowd_bool = crowd_iou_max < 0.001
    else:
        # All anchors don't intersect a crowd
        no_crowd_bool = np.ones([anchors.shape[0]], dtype=bool)

    # Compute overlaps [num_anchors, num_gt_boxes]

    overlaps = utils.compute_overlaps(anchors, gt_boxes)
    #     gt_class_ids_tiled=np.tile(gt_class_ids,(anchors.shape[0],1))

    #     print('overlaps.shape',overlaps.shape,gt_class_ids_tiled.shape)

    # Match anchors to GT Boxes
    # If an anchor overlaps a GT box with IoU >= 0.7 then it's positive.
    # If an anchor overlaps a GT box with IoU < 0.3 then it's negative.
    # Neutral anchors are those that don't match the conditions above,
    # and they don't influence the loss function.
    # However, don't keep any GT box unmatched (rare, but happens). Instead,
    # match it to the closest anchor (even if its max IoU is < 0.3).
    #
    # 1. Set negative anchors first. They get overwritten below if a GT box is
    # matched to them. Skip boxes in crowd areas.
    anchor_iou_argmax = np.argmax(overlaps, axis=1)
    anchor_iou_max = overlaps[np.arange(overlaps.shape[0]), anchor_iou_argmax]

    #     anchor_iou_max_class = gt_class_ids_tiled[np.arange(overlaps.shape[0]), anchor_iou_argmax]
    anchor_iou_max_class = gt_class_ids[anchor_iou_argmax]

    rpn_match[(anchor_iou_max < 0.3) & (no_crowd_bool)] = -1
    #     rpn_match[(anchor_iou_max ==0) & (no_crowd_bool)] = -1

    # 2. Set an anchor for each GT box (regardless of IoU value).
    # If multiple anchors have the same IoU match all of them
    gt_iou_argmax = np.argwhere(overlaps == np.max(overlaps, axis=0))[:, 0]
    rpn_match[gt_iou_argmax] = 1
    rpn_match_class[gt_iou_argmax] = anchor_iou_max_class[gt_iou_argmax]
    # 3. Set anchors with high overlap as positive.
    rpn_match[anchor_iou_max >= 0.7] = 1
    #     rpn_match[anchor_iou_max >= 0.2] = 1

    pos_ids = np.where(rpn_match == 1)[0]
    # Subsample to balance positive and negative anchors
    # Don't let positives be more than half the anchors

    # ids = np.where(rpn_match == 1)[0]
    # extra = len(ids) - (RPN_TRAIN_ANCHORS_PER_IMAGE // 2)
    # if extra > 0:
    #     # Reset the extra ones to neutral
    #     ids = np.random.choice(ids, extra, replace=False)
    #     rpn_match[ids] = 0

    # Same for negative proposals
    # ids = np.where(rpn_match == -1)[0]
    # extra = len(ids) - (RPN_TRAIN_ANCHORS_PER_IMAGE -
    #                     np.sum(rpn_match == 1))
    # if extra > 0:
    #     fgffsgf
    #     # Rest the extra ones to neutral
    #     ids = np.random.choice(ids, extra, replace=False)
    #     rpn_match[ids] = 0

    neg_ids = np.where(rpn_match == -1)[0]
    #     print(neg_ids.shape[0],pos_ids.shape[0])
    if neg_ids.shape[0] > pos_ids.shape[0]:
        #     hgfjh
        ids = np.random.choice(
            neg_ids, neg_ids.shape[0] - pos_ids.shape[0], replace=False
        )
        rpn_match[ids] = 0

    rpn_bbox = np.zeros((pos_ids.shape[0], 4))
    # For positive anchors, compute shift and scale needed to transform them
    # to match the corresponding GT boxes.
    ids = np.where(rpn_match == 1)[0]
    ix = 0  # index into rpn_bbox
    # TODO: use box_refinement() rather than duplicating the code here
    for i, a in zip(ids, anchors[ids]):
        # Closest gt box (it might have IoU < 0.7)
        gt = gt_boxes[anchor_iou_argmax[i]]

        # Convert coordinates to center plus width/height.
        # GT Box
        gt_h = gt[2] - gt[0]
        gt_w = gt[3] - gt[1]
        gt_center_y = gt[0] + 0.5 * gt_h
        gt_center_x = gt[1] + 0.5 * gt_w
        # Anchor
        a_h = a[2] - a[0]
        a_w = a[3] - a[1]
        a_center_y = a[0] + 0.5 * a_h
        a_center_x = a[1] + 0.5 * a_w

        # Compute the bbox refinement that the RPN should predict.
        rpn_bbox[ix] = [
            (gt_center_y - a_center_y) / a_h,
            (gt_center_x - a_center_x) / a_w,
            np.log(gt_h / a_h),
            np.log(gt_w / a_w),
        ]
        # Normalize
        rpn_bbox[ix] /= config.RPN_BBOX_STD_DEV
        ix += 1

    #     print("rpn_match.shape, rpn_bbox.shape") #(5120,) (96, 4)
    #     print(rpn_match.shape, rpn_bbox.shape)

    #     print(anchors[rpn_match==1,:].shape)
    #     print(rpn_bbox)

    #     print(anchor_iou_max_class.shape)

    #     rpn_bbox_reshaped=rpn_bbox.reshape(rpn_bbox.shape[0]*rpn_bbox.shape[1],)
    #     for iii in range(rpn_bbox.shape[0]):
    #         if rpn_match[iii]==1:
    #             print(rpn_match[iii],rpn_bbox_reshaped[iii,:])
    #     print("rpn_bbox",rpn_bbox)
    #     print('rpn_bbox',rpn_bbox.shape,rpn_bbox[0:4,:],np.min(rpn_bbox,axis=0),np.max(rpn_bbox,axis=0))
    #     rpn_match_class=anchor_iou_max_class
    return rpn_match, rpn_bbox, rpn_match_class


def build_rpn_targets(image_shape, anchors, gt_class_ids, gt_boxes, config):
    """Given the anchors and GT boxes, compute overlaps and identify positive
    anchors and deltas to refine them to match their corresponding GT boxes.
    anchors: [num_anchors, (y1, x1, y2, x2)]
    gt_class_ids: [num_gt_boxes] Integer class IDs.
    gt_boxes: [num_gt_boxes, (y1, x1, y2, x2)]
    Returns:
    rpn_match: [N] (int32) matches between anchors and GT boxes.
               1 = positive anchor, -1 = negative anchor, 0 = neutral
    rpn_bbox: [config.RPN_TRAIN_ANCHORS_PER_IMAGE, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.
    """
    # RPN Match: 1 = positive anchor, -1 = negative anchor, 0 = neutral
    rpn_match = np.zeros([anchors.shape[0]], dtype=np.int32)
    rpn_match_class = np.zeros([anchors.shape[0]], dtype=np.int32)
    # RPN bounding boxes: [max anchors per image, (dy, dx, log(dh), log(dw))]
    rpn_bbox = np.zeros((config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4))

    # Handle COCO crowds
    # A crowd box in COCO is a bounding box around several instances. Exclude
    # them from training. A crowd box is given a negative class ID.
    crowd_ix = np.where(gt_class_ids < 0)[0]
    if crowd_ix.shape[0] > 0:
        # Filter out crowds from ground truth class IDs and boxes
        non_crowd_ix = np.where(gt_class_ids > 0)[0]
        crowd_boxes = gt_boxes[crowd_ix]
        gt_class_ids = gt_class_ids[non_crowd_ix]
        gt_boxes = gt_boxes[non_crowd_ix]
        # Compute overlaps with crowd boxes [anchors, crowds]
        crowd_overlaps = utils.compute_overlaps(anchors, crowd_boxes)
        crowd_iou_max = np.amax(crowd_overlaps, axis=1)
        no_crowd_bool = crowd_iou_max < 0.001
    else:
        # All anchors don't intersect a crowd
        no_crowd_bool = np.ones([anchors.shape[0]], dtype=bool)

    # Compute overlaps [num_anchors, num_gt_boxes]

    overlaps = utils.compute_overlaps(anchors, gt_boxes)
    #     gt_class_ids_tiled=np.tile(gt_class_ids,(anchors.shape[0],1))

    #     print('overlaps.shape',overlaps.shape,gt_class_ids_tiled.shape)

    # Match anchors to GT Boxes
    # If an anchor overlaps a GT box with IoU >= 0.7 then it's positive.
    # If an anchor overlaps a GT box with IoU < 0.3 then it's negative.
    # Neutral anchors are those that don't match the conditions above,
    # and they don't influence the loss function.
    # However, don't keep any GT box unmatched (rare, but happens). Instead,
    # match it to the closest anchor (even if its max IoU is < 0.3).
    #
    # 1. Set negative anchors first. They get overwritten below if a GT box is
    # matched to them. Skip boxes in crowd areas.
    anchor_iou_argmax = np.argmax(overlaps, axis=1)
    anchor_iou_max = overlaps[np.arange(overlaps.shape[0]), anchor_iou_argmax]

    #     anchor_iou_max_class = gt_class_ids_tiled[np.arange(overlaps.shape[0]), anchor_iou_argmax]
    anchor_iou_max_class = gt_class_ids[anchor_iou_argmax]

    rpn_match[(anchor_iou_max < 0.3) & (no_crowd_bool)] = -1
    # 2. Set an anchor for each GT box (regardless of IoU value).
    # If multiple anchors have the same IoU match all of them
    gt_iou_argmax = np.argwhere(overlaps == np.max(overlaps, axis=0))[:, 0]
    rpn_match[gt_iou_argmax] = 1
    rpn_match_class[gt_iou_argmax] = anchor_iou_max_class[gt_iou_argmax]
    # 3. Set anchors with high overlap as positive.
    rpn_match[anchor_iou_max >= 0.7] = 1
    #     anchor_iou_max_class[anchor_iou_max < 0.7] = 0

    # Subsample to balance positive and negative anchors
    # Don't let positives be more than half the anchors
    ids = np.where(rpn_match == 1)[0]
    extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE // 2)
    if extra > 0:
        # Reset the extra ones to neutral
        ids = np.random.choice(ids, extra, replace=False)
        rpn_match[ids] = 0
    # Same for negative proposals
    ids = np.where(rpn_match == -1)[0]
    extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE - np.sum(rpn_match == 1))
    if extra > 0:
        # Rest the extra ones to neutral
        ids = np.random.choice(ids, extra, replace=False)
        rpn_match[ids] = 0

    # For positive anchors, compute shift and scale needed to transform them
    # to match the corresponding GT boxes.
    ids = np.where(rpn_match == 1)[0]
    ix = 0  # index into rpn_bbox
    # TODO: use box_refinement() rather than duplicating the code here
    for i, a in zip(ids, anchors[ids]):
        # Closest gt box (it might have IoU < 0.7)
        gt = gt_boxes[anchor_iou_argmax[i]]

        # Convert coordinates to center plus width/height.
        # GT Box
        gt_h = gt[2] - gt[0]
        gt_w = gt[3] - gt[1]
        gt_center_y = gt[0] + 0.5 * gt_h
        gt_center_x = gt[1] + 0.5 * gt_w
        # Anchor
        a_h = a[2] - a[0]
        a_w = a[3] - a[1]
        a_center_y = a[0] + 0.5 * a_h
        a_center_x = a[1] + 0.5 * a_w

        # Compute the bbox refinement that the RPN should predict.
        rpn_bbox[ix] = [
            (gt_center_y - a_center_y) / a_h,
            (gt_center_x - a_center_x) / a_w,
            np.log(gt_h / a_h),
            np.log(gt_w / a_w),
        ]
        # Normalize
        rpn_bbox[ix] /= config.RPN_BBOX_STD_DEV
        ix += 1

    #     print("rpn_match.shape, rpn_bbox.shape") #(5120,) (96, 4)
    #     print(rpn_match.shape, rpn_bbox.shape)

    #     print(anchors[rpn_match==1,:].shape)
    #     print(rpn_bbox)

    #     print(anchor_iou_max_class.shape)

    #     rpn_bbox_reshaped=rpn_bbox.reshape(rpn_bbox.shape[0]*rpn_bbox.shape[1],)
    #     for iii in range(rpn_bbox.shape[0]):
    #         if rpn_match[iii]==1:
    #             print(rpn_match[iii],rpn_bbox_reshaped[iii,:])
    #     print("rpn_bbox",rpn_bbox)
    #     print('rpn_bbox',rpn_bbox.shape,rpn_bbox[0:4,:],np.min(rpn_bbox,axis=0),np.max(rpn_bbox,axis=0))
    #     rpn_match_class=anchor_iou_max_class
    return rpn_match, rpn_bbox, rpn_match_class


# def build_rpn_targets(image_shape, anchors, gt_class_ids, gt_boxes, config):
#     """Given the anchors and GT boxes, compute overlaps and identify positive
#     anchors and deltas to refine them to match their corresponding GT boxes.

#     anchors: [num_anchors, (y1, x1, y2, x2)]
#     gt_class_ids: [num_gt_boxes] Integer class IDs.
#     gt_boxes: [num_gt_boxes, (y1, x1, y2, x2)]

#     Returns:
#     rpn_match: [N] (int32) matches between anchors and GT boxes.
#                1 = positive anchor, -1 = negative anchor, 0 = neutral
#     rpn_bbox: [N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.
#     """
#     # RPN Match: 1 = positive anchor, -1 = negative anchor, 0 = neutral
#     rpn_match = np.zeros([anchors.shape[0]], dtype=np.int32)
#     # RPN bounding boxes: [max anchors per image, (dy, dx, log(dh), log(dw))]
#     rpn_bbox = np.zeros((config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4))

#     # Handle COCO crowds
#     # A crowd box in COCO is a bounding box around several instances. Exclude
#     # them from training. A crowd box is given a negative class ID.
#     crowd_ix = np.where(gt_class_ids < 0)[0]
#     if crowd_ix.shape[0] > 0:
#         # Filter out crowds from ground truth class IDs and boxes
#         non_crowd_ix = np.where(gt_class_ids > 0)[0]
#         crowd_boxes = gt_boxes[crowd_ix]
#         gt_class_ids = gt_class_ids[non_crowd_ix]
#         gt_boxes = gt_boxes[non_crowd_ix]
#         # Compute overlaps with crowd boxes [anchors, crowds]
#         crowd_overlaps = utils.compute_overlaps(anchors, crowd_boxes)
#         crowd_iou_max = np.amax(crowd_overlaps, axis=1)
#         no_crowd_bool = (crowd_iou_max < 0.001)
#     else:
#         # All anchors don't intersect a crowd
#         no_crowd_bool = np.ones([anchors.shape[0]], dtype=bool)

#     # Compute overlaps [num_anchors, num_gt_boxes]
# #     print("anchors, gt_boxes",anchors.shape, gt_boxes.shape)
#     overlaps = utils.compute_overlaps(anchors, gt_boxes)


#     # Match anchors to GT Boxes
#     # If an anchor overlaps a GT box with IoU >= 0.7 then it's positive.
#     # If an anchor overlaps a GT box with IoU < 0.3 then it's negative.
#     # Neutral anchors are those that don't match the conditions above,
#     # and they don't influence the loss function.
#     # However, don't keep any GT box unmatched (rare, but happens). Instead,
#     # match it to the closest anchor (even if its max IoU is < 0.3).
#     #
#     # 1. Set negative anchors first. They get overwritten below if a GT box is
#     # matched to them. Skip boxes in crowd areas.
#     anchor_iou_argmax = np.argmax(overlaps, axis=1)
#     anchor_iou_max = overlaps[np.arange(overlaps.shape[0]), anchor_iou_argmax]
#     rpn_match[(anchor_iou_max < 0.3) & (no_crowd_bool)] = -1
#     # 2. Set an anchor for each GT box (regardless of IoU value).
#     # If multiple anchors have the same IoU match all of them
#     gt_iou_argmax = np.argwhere(overlaps == np.max(overlaps, axis=0))[:,0]
#     rpn_match[gt_iou_argmax] = 1
#     # 3. Set anchors with high overlap as positive.
# #     rpn_match[anchor_iou_max >= 0.7] = 1
#     rpn_match[anchor_iou_max >= 0.9] = 1 #added by marzi

#     # Subsample to balance positive and negative anchors
#     # Don't let positives be more than half the anchors
#     ids = np.where(rpn_match == 1)[0]
#     extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE // 2)
#     if extra > 0:
#         # Reset the extra ones to neutral
#         ids = np.random.choice(ids, extra, replace=False)
#         rpn_match[ids] = 0
#     # Same for negative proposals
#     ids = np.where(rpn_match == -1)[0]
#     extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE -
#                         np.sum(rpn_match == 1))
#     if extra > 0:
#         # Rest the extra ones to neutral
#         ids = np.random.choice(ids, extra, replace=False)
#         rpn_match[ids] = 0

#     # For positive anchors, compute shift and scale needed to transform them
#     # to match the corresponding GT boxes.
#     ids = np.where(rpn_match == 1)[0]
#     ix = 0  # index into rpn_bbox
#     # TODO: use box_refinement() rather than duplicating the code here
#     for i, a in zip(ids, anchors[ids]):
#         # Closest gt box (it might have IoU < 0.7)
#         gt = gt_boxes[anchor_iou_argmax[i]]

#         # Convert coordinates to center plus width/height.
#         # GT Box
#         gt_h = gt[2] - gt[0]
#         gt_w = gt[3] - gt[1]
#         gt_center_y = gt[0] + 0.5 * gt_h
#         gt_center_x = gt[1] + 0.5 * gt_w
#         # Anchor
#         a_h = a[2] - a[0]
#         a_w = a[3] - a[1]
#         a_center_y = a[0] + 0.5 * a_h
#         a_center_x = a[1] + 0.5 * a_w

#         # Compute the bbox refinement that the RPN should predict.
#         rpn_bbox[ix] = [
#             (gt_center_y - a_center_y) / a_h,
#             (gt_center_x - a_center_x) / a_w,
#             np.log(gt_h / a_h),
#             np.log(gt_w / a_w),
#         ]
#         # Normalize
#         rpn_bbox[ix] /= config.RPN_BBOX_STD_DEV
#         ix += 1

# #     print("rpn_bbox",rpn_bbox)
#     print("rpn_match[ii,:],rpn_bbox[ii,:]")
#     for iii in range(rpn_bbox.shape[0]):
#         if rpn_match[iii]==1:
#             print(rpn_match[iii],rpn_bbox[iii,:])

#     return rpn_match, rpn_bbox


def generate_random_rois(image_shape, count, gt_class_ids, gt_boxes):
    """Generates ROI proposals similar to what a region proposal network
    would generate.

    image_shape: [Height, Width, Depth]
    count: Number of ROIs to generate
    gt_class_ids: [N] Integer ground truth class IDs
    gt_boxes: [N, (y1, x1, y2, x2)] Ground truth boxes in pixels.

    Returns: [count, (y1, x1, y2, x2)] ROI boxes in pixels.
    """
    # placeholder
    rois = np.zeros((count, 4), dtype=np.int32)

    # Generate random ROIs around GT boxes (90% of count)
    rois_per_box = int(0.9 * count / gt_boxes.shape[0])
    for i in range(gt_boxes.shape[0]):
        gt_y1, gt_x1, gt_y2, gt_x2 = gt_boxes[i]
        h = gt_y2 - gt_y1
        w = gt_x2 - gt_x1
        # random boundaries
        r_y1 = max(gt_y1 - h, 0)
        r_y2 = min(gt_y2 + h, image_shape[0])
        r_x1 = max(gt_x1 - w, 0)
        r_x2 = min(gt_x2 + w, image_shape[1])

        # To avoid generating boxes with zero area, we generate double what
        # we need and filter out the extra. If we get fewer valid boxes
        # than we need, we loop and try again.
        while True:
            y1y2 = np.random.randint(r_y1, r_y2, (rois_per_box * 2, 2))
            x1x2 = np.random.randint(r_x1, r_x2, (rois_per_box * 2, 2))
            # Filter out zero area boxes
            threshold = 1
            y1y2 = y1y2[np.abs(y1y2[:, 0] - y1y2[:, 1]) >= threshold][:rois_per_box]
            x1x2 = x1x2[np.abs(x1x2[:, 0] - x1x2[:, 1]) >= threshold][:rois_per_box]
            if y1y2.shape[0] == rois_per_box and x1x2.shape[0] == rois_per_box:
                break

        # Sort on axis 1 to ensure x1 <= x2 and y1 <= y2 and then reshape
        # into x1, y1, x2, y2 order
        x1, x2 = np.split(np.sort(x1x2, axis=1), 2, axis=1)
        y1, y2 = np.split(np.sort(y1y2, axis=1), 2, axis=1)
        box_rois = np.hstack([y1, x1, y2, x2])
        rois[rois_per_box * i : rois_per_box * (i + 1)] = box_rois

    # Generate random ROIs anywhere in the image (10% of count)
    remaining_count = count - (rois_per_box * gt_boxes.shape[0])
    #     print(count,rois_per_box,gt_boxes.shape[0],"remaining_count",remaining_count)
    # To avoid generating boxes with zero area, we generate double what
    # we need and filter out the extra. If we get fewer valid boxes
    # than we need, we loop and try again.
    while True:
        y1y2 = np.random.randint(0, image_shape[0], (remaining_count * 2, 2))
        x1x2 = np.random.randint(0, image_shape[1], (remaining_count * 2, 2))
        # Filter out zero area boxes
        threshold = 1
        y1y2 = y1y2[np.abs(y1y2[:, 0] - y1y2[:, 1]) >= threshold][:remaining_count]
        x1x2 = x1x2[np.abs(x1x2[:, 0] - x1x2[:, 1]) >= threshold][:remaining_count]
        if y1y2.shape[0] == remaining_count and x1x2.shape[0] == remaining_count:
            break

    # Sort on axis 1 to ensure x1 <= x2 and y1 <= y2 and then reshape
    # into x1, y1, x2, y2 order
    x1, x2 = np.split(np.sort(x1x2, axis=1), 2, axis=1)
    y1, y2 = np.split(np.sort(y1y2, axis=1), 2, axis=1)
    global_rois = np.hstack([y1, x1, y2, x2])
    rois[-remaining_count:] = global_rois
    return rois


############################################################
#  Data Formatting
############################################################


def compose_image_meta(
    image_id,
    original_image_shape,
    image_shape,
    window,
    scale,
    active_class_ids,
    img_site_and_epoch,
):
    """Takes attributes of an image and puts them in one 1D array.

    image_id: An int ID of the image. Useful for debugging.
    original_image_shape: [H, W, C] before resizing or padding.
    image_shape: [H, W, C] after resizing and padding
    window: (y1, x1, y2, x2) in pixels. The area of the image where the real
            image is (excluding the padding)
    scale: The scaling factor applied to the original image (float32)
    active_class_ids: List of class_ids available in the dataset from which
        the image came. Useful if training on images from multiple datasets
        where not all classes are present in all datasets.
    img_site_and_epoch:

    """
    meta = np.array(
        [image_id]
        + list(original_image_shape)  # size=1
        + list(image_shape)  # size=3
        + list(window)  # size=3
        + [scale]  # size=4 (y1, x1, y2, x2) in image cooredinates
        + list(active_class_ids)  # size=1
        + img_site_and_epoch  # size=num_classes
    )
    return meta


def parse_image_meta(meta):
    """Parses an array that contains image attributes to its components.
    See compose_image_meta() for more details.

    meta: [batch, meta length] where meta length depends on NUM_CLASSES

    Returns a dict of the parsed values.
    """
    image_id = meta[:, 0]
    original_image_shape = meta[:, 1:4]
    image_shape = meta[:, 4:7]
    window = meta[:, 7:11]  # (y1, x1, y2, x2) window of image in in pixels
    scale = meta[:, 11]
    active_class_ids = meta[:, 12:-2]
    im_site_epoch = meta[:, -2]
    return {
        "image_id": image_id.astype(np.int32),
        "original_image_shape": original_image_shape.astype(np.int32),
        "image_shape": image_shape.astype(np.int32),
        "window": window.astype(np.int32),
        "scale": scale.astype(np.float32),
        "active_class_ids": active_class_ids.astype(np.int32),
        "im_site_epoch": im_site_epoch.astype(np.int32),
    }


def parse_image_meta_graph(meta):
    """Parses a tensor that contains image attributes to its components.
    See compose_image_meta() for more details.

    meta: [batch, meta length] where meta length depends on NUM_CLASSES

    Returns a dict of the parsed tensors.
    """
    image_id = meta[:, 0]
    original_image_shape = meta[:, 1:4]
    image_shape = meta[:, 4:7]
    window = meta[:, 7:11]  # (y1, x1, y2, x2) window of image in in pixels
    scale = meta[:, 11]
    active_class_ids = meta[:, 12:-2]
    im_site_epoch = meta[:, -2]
    #     print("active_class_ids",active_class_ids)
    return {
        "image_id": image_id,
        "original_image_shape": original_image_shape,
        "image_shape": image_shape,
        "window": window,
        "scale": scale,
        "active_class_ids": active_class_ids,
        "im_site_epoch": im_site_epoch,
    }


def mold_image(images, config):
    """Expects an RGB image (or array of images) and subtracts
    the mean pixel and converts it to float. Expects image
    colors in RGB order.
    """
    return images.astype(np.float32) - config.MEAN_PIXEL


def unmold_image(normalized_images, config):
    """Takes a image normalized with mold() and returns the original."""
    return (normalized_images + config.MEAN_PIXEL).astype(np.uint8)


def generate_gt_rois(image_shape, count, gt_class_ids, gt_boxes):
    """Generates ROI proposals converting GT boxes into proposaly. use same output
    format as RPN network layer has. We add one backgroung proposals. This is
    used later to fill up the proposals list

    image_shape: [Height, Width, Depth]
    gt_class_ids: [N] Integer ground truth class IDs
    gt_boxes: [N, (y1, x1, y2, x2)] Ground truth boxes in pixels.
    mode: only inference mode will be accepted
    Returns: [count, (y1, x1, y2, x2)] ROI boxes in pixels.
    """

    # placeholder
    rois = np.zeros((count, 4), dtype=np.int32)

    # Simply copy ground truth boxes over to rois boxes as foregrounds
    for i in range(gt_boxes.shape[0]):
        gt_y1, gt_x1, gt_y2, gt_x2 = gt_boxes[i]
        box_rois = np.hstack([gt_y1, gt_x1, gt_y2, gt_x2])
        rois[i] = box_rois

    # We add up the rest with background item, very small. These
    # proposals will later fall out anyway.
    for i in range(gt_boxes.shape[0], count):
        back_rois = np.hstack([0, 0, 5, 5])
        rois[i] = back_rois

    return rois
