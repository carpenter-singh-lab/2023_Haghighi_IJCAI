"""
[The base of this script is Matterport implementation of Mask R-CNN model Written by Waleed Abdulla (Copyright (c) 2017 Matterport, Inc.).]


Data Generator functions are very important in our model since we have huge number of objects and handling data is important

List of generators and their properties:


------------------------------------------------------


* DataGenerator (class)
  - def __init__(self,dataset_inp, config, shuffle=False, augment=False, augmentation=None,
                   random_rois=0, batch_size=1, detection_targets=False,
                   no_augmentation_sources=None):
  - def __len__(self):
  - def __getitem__(self, index):
    which calls:
    - def __data_generation(self, dataset, batch_image_ids):
      - def load_image_gt(dataset, config, image_id, augment=False, augmentation=None,use_mini_mask=False)
               -  image = dataset.load_image_cr_presaved(image_id,config.batchplate_well) or\
                  image = dataset.load_image(image_id)
               -  mask, class_ids = dataset.load_mask(image_id) or mask, class_ids = dataset.create_mask(image_id,image)
               - replaced with: bbox, class_ids = dataset.load_bbox(image_id)
         return image, image_meta, class_ids, bbox, mask
      
      - def build_rpn_targets(image_shape, anchors, gt_class_ids, gt_boxes, config)
          return rpn_match, rpn_bbox, rpn_match_class
      
      
      - def mold_image(images, config)
      
  - def on_epoch_end(self):
  
  -> Operation algorithm:
     This data generator is designed so that it goes through images of one site in each epoch,
     and in each step or iteration inside each epoch it goes through all the cropped sequences of 
     that site (n=484 max if crops are 256x256 - image_ids 484*9=4356 max) 
     Therefore when using this data generator, epochs must be more than number of sites in each well 
     and STEPS_PER_EPOCH should be more than number of cropped sequences (here 484).
     We dont select sites randomly from the dataset_inp list since that way we need MANY epochs to 
     ensure going through the whole list of sites and generate barcodes for all!



* DataGenerator_test (class)

* DataGeneratorBatch (class)  --- is called when -> config.load_batch_images = True
  This class is for when we want to create locations on the fly by thresholding the max-projected 
  image. We need another generator which takes all the 9 cycles images together for creating the gt_ 
  variables in each batch.
  
  Here, __data_generation is differnt than DataGenerator class. 
  - def __data_generation(self, dataset, batch_image_ids):
      - def load_batch_ims_cr_presaved(self, image_id,batchplate_well):
      - def create_mask_batch(self,images4D):

 

------------------------------------------------------


* data_generator (function)
  
* data_generator_test (function)


Author: 
Marzieh Haghighi
"""


import math
import pdb
import skimage.io
import pandas as pd

import numpy as np
import tensorflow.keras as keras
import tensorflow.keras.utils as KU

import barcodefit.model.model_utils as utils
from barcodefit.dataobjects import spot

seed = 42

# Set seed for NumPy
np.random.seed(seed)


# -----------------------------------------------------------------------------------------
# from tensorflow.keras.utils import Sequence
class DataGenerator(KU.Sequence):
    """Generates data for Keras Sequence based data generator.
    Suitable for building data generator for training and prediction.
    """

    def __init__(
        self,
        list_of_sites,
        config,
        shuffle=False,
        augment=False,
        augmentation=None,
        random_rois=0,
        true_rois=False,
        batch_size=1,
        detection_targets=False,
        no_augmentation_sources=None,
    ):

        """Initialization
        dataset_inp: is a list of dataobjects for all sites -> len=number of sites
        dataset: The Dataset object to pick data from
        config: The model config object
        shuffle: If True, shuffles the samples before every epoch
        batch_size: How many images to return in each call
        detection_targets: If True, generate detection targets (class IDs, bbox
            deltas, and masks). Typically for debugging or visualizations because
            in trainig detection targets are generated by DetectionTargetLayer.
        """

        if not config.USE_RPN_ROIS:
            self.true_rois = config.POST_NMS_ROIS_TRAINING
        else:
            self.true_rois = true_rois

        self.epoch = -1
        self.site_step = config.starting_site  # -1#20
        # self.dataset_inp = dataset_inp
        self.list_of_sites= list_of_sites
        self.n_total_sites=len(list_of_sites)
        self.config = config
        self.augment = augment
        self.augmentation = augmentation
        self.random_rois = random_rois
        #         self.random_rois = 500  #changed by Marzi
        self.batch_size = batch_size
        self.detection_targets = detection_targets
        self.no_augmentation_sources = no_augmentation_sources or []

        # self.indexes = np.arange(0, len(dataset_inp[0].image_ids), self.batch_size)
        # Anchors
        # [anchor_count, (y1, x1, y2, x2)]
        self.backbone_shapes = compute_backbone_shapes(
            self.config, self.config.IMAGE_SHAPE
        )
        print("self.backbone_shapes", self.backbone_shapes)
        self.anchors = utils.generate_pyramid_anchors(
            self.config.RPN_ANCHOR_SCALES,
            self.config.RPN_ANCHOR_RATIOS,
            self.backbone_shapes,
            self.config.BACKBONE_STRIDES,
            self.config.RPN_ANCHOR_STRIDE,
        )

        self.on_epoch_end()

    #         self.on_test_end()

    def __len__(self):
        """Denotes the number of batches per epoch
        :return: number of batches per epoch
        """
        return self.config.STEPS_PER_EPOCH

    def __getitem__(self, index):
        """Generate one batch of data

        index: index of the batch
        return: X and y when fitting. X only when predicting
        """
        # Generate indexes of the batch by checking the current site first

        #         dataset=random.choice(dataset_inp); # for when we want to randomly select a site

        #         When site_step or epoch exceeds number of sites in the list of dataset_inp, we again go through
        #         the list from start
        # dataset = self.dataset_inp[self.site_step % self.n_total_sites]
        dataset=self.ds_site

        #         print('epoch: ', self.epoch)
        #         print('Site: ', int(dataset.image_info[0]["site"]))

        # image ids for each site which are basically ids for crops
        image_ids = np.copy(dataset.image_ids)

        index = index % len(self.indexes)

        image_index_firstSeq = self.indexes[index]

        batch_image_ids = image_ids[
            image_index_firstSeq : image_index_firstSeq + self.batch_size
        ]

        inputs, outputs = self.__data_generation(dataset, batch_image_ids)

        return inputs, outputs

    def __data_generation(self, dataset, batch_image_ids):
        "Generates data containing batch_size samples"  # X : (n_samples, *dim, n_channels)

        batch_size = self.batch_size
        b = 0
        #         print(self.epoch)

        for image_id in batch_image_ids:
            #             print('image_id',image_id)

            #             image, image_meta, gt_class_ids, gt_boxes, gt_masks = \
            #             load_image_gt(dataset, self.config, image_id, augment=self.augment,
            #                           augmentation=None,
            #                           use_mini_mask=self.config.USE_MINI_MASK,epoch=self.epoch)

            image, image_meta, gt_class_ids, gt_boxes = load_image_gt(
                dataset,
                self.config,
                image_id,
                augment=self.augment,
                augmentation=None,
                use_mini_mask=self.config.USE_MINI_MASK,
                site_image=self.site_image,
                epoch=self.epoch,
            )

            #             print(image_meta.shape)

            # Skip images that have no instances. This can happen in cases
            # where we train on a subset of classes and the image doesn't
            # have any of the classes we care about.
            if not np.any(gt_class_ids > 0):
                continue

            # RPN Targets
            if self.config.rpn_clustering:
                rpn_match, rpn_bbox, rpn_match_class = build_rpn_targets_marzi(
                    image.shape, self.anchors, gt_class_ids, gt_boxes, self.config
                )
            #                 print(rpn_match.shape, rpn_bbox.shape, rpn_match_class.shape,gt_class_ids.shape, gt_boxes.shape)
            else:
                rpn_match, rpn_bbox, rpn_match_class = build_rpn_targets(
                    image.shape, self.anchors, gt_class_ids, gt_boxes, self.config
                )

            #             rpn_match, rpn_bbox ,anchors= build_rpn_targets(image.shape, anchors,
            #                                                     gt_class_ids, gt_boxes, config) # added by Marzi

            #             print("rpn_bbox",rpn_bbox.shape)#(96, 4)
            #             print("rpn_match",rpn_bbox.shape) (96, 4)

            # Mask R-CNN Targets
            if self.random_rois:
                rpn_rois = generate_random_rois(
                    image.shape, self.random_rois, gt_class_ids, gt_boxes
                )
                if self.detection_targets:
                    (
                        rois,
                        mrcnn_class_ids,
                        mrcnn_bbox,
                        mrcnn_mask,
                    ) = build_detection_targets(
                        rpn_rois, gt_class_ids, gt_boxes, gt_masks, self.config
                    )

            # Mask R-CNN GT targets
            if self.true_rois:
                rpn_rois = generate_gt_rois(
                    image.shape, self.true_rois, gt_class_ids, gt_boxes
                )

            # Init batch arrays
            if b == 0:
                batch_image_meta = np.zeros(
                    (batch_size,) + image_meta.shape, dtype=image_meta.dtype
                )
                batch_rpn_match = np.zeros(
                    [batch_size, self.anchors.shape[0], 1], dtype=rpn_match.dtype
                )
                batch_rpn_match_class = np.zeros(
                    [batch_size, self.anchors.shape[0], 1], dtype=rpn_match_class.dtype
                )
                if self.config.rpn_clustering:
                    batch_rpn_bbox = np.zeros(
                        [batch_size, rpn_bbox.shape[0], 4], dtype=rpn_bbox.dtype
                    )
                else:
                    batch_rpn_bbox = np.zeros(
                        [batch_size, self.config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4],
                        dtype=rpn_bbox.dtype,
                    )

                batch_images = np.zeros((batch_size,) + image.shape, dtype=np.float32)
                batch_gt_class_ids = np.zeros(
                    (batch_size, self.config.MAX_GT_INSTANCES), dtype=np.int32
                )
                batch_gt_boxes = np.zeros(
                    (batch_size, self.config.MAX_GT_INSTANCES, 4), dtype=np.int32
                )
                #                 batch_gt_masks = np.zeros(
                #                     (batch_size, gt_masks.shape[0], gt_masks.shape[1],
                #                      self.config.MAX_GT_INSTANCES), dtype=gt_masks.dtype)

                if self.true_rois:
                    batch_rpn_rois = np.zeros(
                        (batch_size, rpn_rois.shape[0], 4), dtype=rpn_rois.dtype
                    )

                if self.random_rois:
                    batch_rpn_rois = np.zeros(
                        (batch_size, rpn_rois.shape[0], 4), dtype=rpn_rois.dtype
                    )
                    if self.detection_targets:
                        batch_rois = np.zeros(
                            (batch_size,) + rois.shape, dtype=rois.dtype
                        )
                        batch_mrcnn_class_ids = np.zeros(
                            (batch_size,) + mrcnn_class_ids.shape,
                            dtype=mrcnn_class_ids.dtype,
                        )
                        batch_mrcnn_bbox = np.zeros(
                            (batch_size,) + mrcnn_bbox.shape, dtype=mrcnn_bbox.dtype
                        )
            #                         batch_mrcnn_mask = np.zeros(
            #                             (batch_size,) + mrcnn_mask.shape, dtype=mrcnn_mask.dtype)

            # If more instances than fits in the array, sub-sample from them.
            if gt_boxes.shape[0] > self.config.MAX_GT_INSTANCES:
                #                 ids = np.random.choice(
                #                     np.arange(gt_boxes.shape[0]), config.MAX_GT_INSTANCES, replace=False)
                #                 gt_class_ids = gt_class_ids[ids]
                gt_class_ids = gt_class_ids[0 : self.config.MAX_GT_INSTANCES]
                gt_boxes = gt_boxes[0 : self.config.MAX_GT_INSTANCES]
            #                 gt_masks = gt_masks[:, :, 0:self.config.MAX_GT_INSTANCES]

            # Add to batch
            batch_image_meta[b] = image_meta
            batch_rpn_match[b] = rpn_match[:, np.newaxis]
            batch_rpn_match_class[b] = rpn_match_class[:, np.newaxis]
            if rpn_bbox.shape[0] >= batch_rpn_bbox.shape[1]:
                batch_rpn_bbox[b] = rpn_bbox[: batch_rpn_bbox.shape[1], :]
            else:
                batch_rpn_bbox[b, : rpn_bbox.shape[0], :] = rpn_bbox
            batch_images[b] = mold_image(image.astype(np.float32), self.config)
            batch_gt_class_ids[b, : gt_class_ids.shape[0]] = gt_class_ids
            batch_gt_boxes[b, : gt_boxes.shape[0]] = gt_boxes
            #             batch_gt_masks[b, :, :, :gt_masks.shape[-1]] = gt_masks

            if self.true_rois:
                batch_rpn_rois[b] = rpn_rois

            if self.random_rois:
                batch_rpn_rois[b] = rpn_rois
                if self.detection_targets:
                    batch_rois[b] = rois
                    batch_mrcnn_class_ids[b] = mrcnn_class_ids
                    batch_mrcnn_bbox[b] = mrcnn_bbox
            #                     batch_mrcnn_mask[b] = mrcnn_mask
            b += 1

        # Batch full?
        #         if b >= batch_size:
        #         inputs = [batch_images, batch_image_meta, batch_rpn_match, batch_rpn_bbox,
        #                   batch_gt_class_ids, batch_gt_boxes, batch_gt_masks]

        inputs = [
            batch_images,
            batch_image_meta,
            batch_rpn_match,
            batch_rpn_bbox,
            batch_gt_class_ids,
            batch_gt_boxes,
            batch_rpn_match_class,
        ]
        #                 print("batch_gt_boxes",batch_gt_boxes)
        outputs = []
        if self.true_rois:
            inputs.extend([batch_rpn_rois])

        if self.random_rois:
            inputs.extend([batch_rpn_rois])
            if self.detection_targets:
                inputs.extend([batch_rois])
                # Keras requires that output and targets have the same number of dimensions
                batch_mrcnn_class_ids = np.expand_dims(batch_mrcnn_class_ids, -1)
                outputs.extend([batch_mrcnn_class_ids, batch_mrcnn_bbox])
        #         print("batch_images min max",batch_images.min(),batch_images.max())
        return inputs, outputs

    def on_epoch_end(self):
        """Updates indexes after each epoch"""
        #         self.indexes = np.arange(len(self.list_IDs))
        self.site_step += 1
        self.epoch += 1
        
        site = self.list_of_sites[self.site_step % self.n_total_sites]
        self.site_image=load_site_tiff_raw_images(self.config.im_Dir,self.config.batchplate_well, site)       
        
        self.ds_site=load_site_metadata_online(self.config.dl_meta_Dir,self.config.batchplate_well,\
                                               site,self.config.IMAGES_PER_GPU);        
        
        self.indexes = np.arange(0,len(self.ds_site.image_ids),self.batch_size)
        
 
        

    def on_test_batch_end(self):
        self.site_step += 1
        self.epoch += 1
        self.indexes = np.arange(
            0,
            len(self.dataset_inp[self.site_step % self.n_total_sites].image_ids),
            self.batch_size,
        )


#     def on_test_end(self):
#         self.site_step+=1
# #         self.epoch+=1
#         print("site_step",self.site_step)
#         self.indexes = np.arange(0,len(self.dataset_inp[self.site_step % self.n_total_sites].image_ids),self.batch_size)




############################################################
#  Data Generator req functions
############################################################


def load_image_gt(
    dataset,
    config,
    image_id,
    augment=False,
    augmentation=None,
    use_mini_mask=False,
    site_image=None,
    epoch=0,
):
    """Load and return ground truth data for an image (image, bounding boxes).

    Returns:
    image: [height, width, 3]
    shape: the original shape of the image before resizing and cropping.
    class_ids: [instance_count] Integer class IDs
    bbox: [instance_count, (y1, x1, y2, x2)]

    """

    # Load image and mask
    #     image = dataset.load_image(image_id)

    if config.load_cropped_presaved:
        image = dataset.load_image_cr_presaved(image_id, config.batchplate_well)
    elif config.load_tiff_crop_online:
        image = dataset.load_image_memmap(image_id,site_image,config.IMAGES_PER_GPU)
    else:
        image = dataset.load_image(image_id)

    #     pdb.set_trace()

    if config.create_mask:
        bbox, class_ids = dataset.create_bbox(image)

    else:
        bbox, class_ids = dataset.load_bbox(image_id)

    original_shape = image.shape
    image, window, scale, padding, crop = utils.resize_image(
        image,
        min_dim=config.IMAGE_MIN_DIM,
        min_scale=config.IMAGE_MIN_SCALE,
        max_dim=config.IMAGE_MAX_DIM,
        mode=config.IMAGE_RESIZE_MODE,
    )

    if (config.IMAGE_MIN_DIM != original_shape[0]) or (
        config.IMAGE_MAX_DIM != original_shape[0]
    ):
        raise Exception(
            "Input Images should be square and resizing is not implemented, to do this make sure you are resizing bbox variable!"
        )

    active_class_ids = np.zeros(
        [dataset.num_classes], dtype=np.int32
    )  # commented by Marzi
    #     active_class_ids = np.zeros([config.NUM_CLASSES], dtype=np.int32)
    source_class_ids = dataset.source_class_ids[dataset.image_info[image_id]["source"]]
    active_class_ids[source_class_ids] = 1

    #     img_site=int(dataset.image_info[image_id]["overlay_dir"].split('_')[-2])
    img_site = dataset.image_info[image_id]["site"]
    # Image meta data
    image_meta = compose_image_meta(
        image_id,
        original_shape,
        image.shape,
        window,
        scale,
        active_class_ids,
        [img_site, epoch],
    )

    return image, image_meta, class_ids, bbox


def load_site_tiff_raw_images(im_Dir, batchplate_well, site):
     
    parent_folder=im_Dir+batchplate_well #cp228    
        
        
    channels = ["C","G","A","T"]

    # Preallocate array
    images4D = np.zeros((9, 5500, 5500, len(channels)), dtype=np.uint8)

    for cycle in range(9):
        for channel_idx, channel in enumerate(channels):
            # Form the filename
            filename = f'{parent_folder}/Cycle0{cycle+1}_{channel}/Cycle0{cycle+1}_{channel}_Site_{site}.tiff'

            # Load the image
            im_uint16 = skimage.io.imread(filename).squeeze()

            # Normalize the image to 8-bit
            im_max_2scale = im_uint16.max()
            im_uint8 = ((im_uint16 / im_max_2scale) * 255).astype(np.uint8)

            # Add the image to our 4D array
            images4D[cycle, :, :, channel_idx] = im_uint8

    return images4D

def load_site_metadata_online2(meta_Dir, batchplate_well, site, n_seq):
    # Read CSV data
    dfInfo3 = pd.read_csv(f'{meta_Dir}df_Info_pcp_{batchplate_well}_{site}_cp.csv')

    # Expand the compact format
    dfInfo3 = pd.concat([dfInfo3.assign(Metadata_Cycle=i+1) for i in range(n_seq)]).reset_index(drop=True)

    # Prepare new columns
    dfInfo3["image_id2"] = dfInfo3["image_id"]
    dfInfo3["cat_id"] = dfInfo3.apply(lambda x: eval(x['BarcodeList_cat_id'])[x['Metadata_Cycle']-1], axis=1)

    # Update image_id based on unique figures
    uniq_figs = dfInfo3.groupby(["image_id2","Metadata_Cycle"]).ngroup()
    dfInfo3["image_id"] = uniq_figs

    # Prepare mapping dictionaries
    map_dict = {'A':3,'T':4,'G':2,'C':1}
    reverse_map_dict = {v: k for k, v in map_dict.items()}

    # Apply the reverse mapping
    dfInfo3["Metadata_Label"] = dfInfo3["cat_id"].astype(str).str[0].map(reverse_map_dict)

    # Update selected columns
    dfInfo3[["bbox", "Location_Center_X", "Location_Center_Y"]] = dfInfo3[["bbox1", "Location_Center_X1", "Location_Center_Y1"]]

    # Assign subset_label to "train" directly
    dfInfo3['subset_label'] = "train"

    # Prepare the dataset
    dataset_train_site = spot.spotsDataset()
    dataset_train_site.load_spots(dfInfo3, "train")
    dataset_train_site.prepare()

    return dataset_train_site


def load_site_metadata_online(meta_Dir, batchplate_well, site, n_seq):
    # Read CSV data
    dfInfo3 = pd.read_csv(f'{meta_Dir}df_Info_pcp_{batchplate_well}_{site}_cp.csv')

    # Expand the compact format
    dfInfo3 = pd.concat([dfInfo3.assign(Metadata_Cycle=i+1) for i in range(n_seq)]).reset_index(drop=True)

    # Prepare new columns
    dfInfo3["image_id2"] = dfInfo3["image_id"]
    dfInfo3["cat_id"] = dfInfo3.apply(lambda x: eval(x['BarcodeList_cat_id'])[x['Metadata_Cycle']-1], axis=1)

    # Update image_id based on unique figures
    uniq_figs = dfInfo3.groupby(["image_id2","Metadata_Cycle"]).ngroup()
    dfInfo3["image_id"] = uniq_figs

    # Prepare mapping dictionaries
    map_dict = {'A':3,'T':4,'G':2,'C':1}
    reverse_map_dict = {v: k for k, v in map_dict.items()}

    # Apply the reverse mapping
    dfInfo3["Metadata_Label"] = dfInfo3["cat_id"].astype(str).str[0].map(reverse_map_dict)

    # Update selected columns
    dfInfo3[["bbox", "Location_Center_X", "Location_Center_Y"]] = dfInfo3[["bbox1", "Location_Center_X1", "Location_Center_Y1"]]

    # Assign subset_label to "train" directly
    dfInfo3['subset_label'] = "train"

    # Prepare the dataset
    dataset_train_site = spot.spotsDataset()
    dataset_train_site.load_spots(dfInfo3, "train")
    dataset_train_site.prepare()

    return dataset_train_site


# def load_site_metadata_online(meta_Dir,batchplate_well, site,n_seq):
    
#     dfInfo3 = pd.read_csv(meta_Dir+'df_Info_pcp_'+batchplate_well+'_'+str(site)+'_cp.csv') 
# #     dfInfo3['im_paths']=dfInfo3['im_paths'].apply(lambda x: correct_address(eval(x)))
    
# #     dfInfo3=dfInfo3.rename(columns={"Barcode_BarcodeCalled":"Barcode_BarcodeCalled_simple"})
# #     dfInfo3=dfInfo3.rename(columns={"Barcode_BarcodeCalled_cp":"Barcode_BarcodeCalled"})    
    
#         ###### Expand the compact format
#     list_ofCycles=[]

#     # dfInfo42=dfInfo3.copy()
#     for i in range(n_seq):
# #         print(i)
#         dfInfo42=dfInfo3.copy()
#         dfInfo42['Metadata_Cycle']=i+1
#         list_ofCycles.append(dfInfo42);

#     dfInfo=pd.concat(list_ofCycles).reset_index(drop=True)

#     dfInfo["image_id2"]=dfInfo["image_id"]

#     dfInfo["cat_id"]=dfInfo.apply(lambda x: eval(x['BarcodeList_cat_id'])[x['Metadata_Cycle']-1], axis=1)
# #     dfInfo["image_id"]=dfInfo.apply(lambda x: x['image_id']+(x['Metadata_Cycle']-1)*323, axis=1)

#     dfInfo["image_id"]=np.nan
#     uniq_figs=dfInfo.groupby(["image_id2","Metadata_Cycle"]).size().reset_index()
#     for r in range(uniq_figs.shape[0]):
#         imi,cy=uniq_figs.loc[r,["image_id2","Metadata_Cycle"]].values
#         dfInfo.loc[(dfInfo["image_id2"]==imi) & (dfInfo["Metadata_Cycle"]==cy),"image_id"]=r


# #         dfInfo["im_paths"]=dfInfo.apply(lambda x: str([y.replace("1_",str(x['Metadata_Cycle'])+"_") for y in eval(x['im_paths'])]), axis=1)

# #     if batch_abbrv=='CP074':
# #         map_dict={'A':1,'T':2,'C':3,'G':4} #was used for cp074
# #     elif batch_abbrv=='CP228':   
#     map_dict={'A':3,'T':4,'G':2,'C':1} #was used for cp0228  
# #     im_orig_size=5500


#     reverse_map_dict = {v: k for k, v in map_dict.items()}

#     dfInfo["Metadata_Label"]=dfInfo["cat_id"].apply(lambda x: list(map(reverse_map_dict.get, str(x)))[0])

#     # # ###############


#     dfInfo["bbox"]=dfInfo["bbox1"]
#     dfInfo["Location_Center_X"]=dfInfo["Location_Center_X1"]
#     dfInfo["Location_Center_Y"]=dfInfo["Location_Center_Y1"]

#     ######### partition dataset to train and validation
#     trainImIds=set(dfInfo['image_id'].unique().tolist())

#     dfInfo.loc[dfInfo['image_id'].isin(trainImIds),'subset_label']="train"

#     dataset_train_site = spot.spotsDataset()
#     dataset_train_site.load_spots(dfInfo,"train")
#     dataset_train_site.prepare()

               
#     return dataset_train_site




def compute_backbone_shapes(config, image_shape):
    """Computes the width and height of each stage of the backbone network.

    Returns:
        [N, (height, width)]. Where N is the number of stages
    """
    if callable(config.BACKBONE):
        return config.COMPUTE_BACKBONE_SHAPE(image_shape)

    # Currently supports ResNet only
    assert config.BACKBONE in ["resnet18", "resnet50", "resnet101"]
    return np.array(
        [
            [
                int(math.ceil(image_shape[0] / stride)),
                int(math.ceil(image_shape[1] / stride)),
            ]
            for stride in config.BACKBONE_STRIDES
        ]
    )


def build_detection_targets(rpn_rois, gt_class_ids, gt_boxes, gt_masks, config):
    """Generate targets for training Stage 2 classifier and mask heads.
    This is not used in normal training. It's useful for debugging or to train
    the Mask RCNN heads without using the RPN head.

    Inputs:
    rpn_rois: [N, (y1, x1, y2, x2)] proposal boxes.
    gt_class_ids: [instance count] Integer class IDs
    gt_boxes: [instance count, (y1, x1, y2, x2)]
    gt_masks: [height, width, instance count] Ground truth masks. Can be full
              size or mini-masks.

    Returns:
    rois: [TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)]
    class_ids: [TRAIN_ROIS_PER_IMAGE]. Integer class IDs.
    bboxes: [TRAIN_ROIS_PER_IMAGE, NUM_CLASSES, (y, x, log(h), log(w))]. Class-specific
            bbox refinements.
    masks: [TRAIN_ROIS_PER_IMAGE, height, width, NUM_CLASSES). Class specific masks cropped
           to bbox boundaries and resized to neural network output size.
    """
    assert rpn_rois.shape[0] > 0
    assert gt_class_ids.dtype == np.int32, "Expected int but got {}".format(
        gt_class_ids.dtype
    )
    assert gt_boxes.dtype == np.int32, "Expected int but got {}".format(gt_boxes.dtype)
    assert gt_masks.dtype == np.bool_, "Expected bool but got {}".format(gt_masks.dtype)

    # It's common to add GT Boxes to ROIs but we don't do that here because
    # according to XinLei Chen's paper, it doesn't help.

    # Trim empty padding in gt_boxes and gt_masks parts
    instance_ids = np.where(gt_class_ids > 0)[0]
    assert instance_ids.shape[0] > 0, "Image must contain instances."
    gt_class_ids = gt_class_ids[instance_ids]
    gt_boxes = gt_boxes[instance_ids]
    gt_masks = gt_masks[:, :, instance_ids]

    # Compute areas of ROIs and ground truth boxes.
    rpn_roi_area = (rpn_rois[:, 2] - rpn_rois[:, 0]) * (rpn_rois[:, 3] - rpn_rois[:, 1])
    gt_box_area = (gt_boxes[:, 2] - gt_boxes[:, 0]) * (gt_boxes[:, 3] - gt_boxes[:, 1])

    # Compute overlaps [rpn_rois, gt_boxes]
    overlaps = np.zeros((rpn_rois.shape[0], gt_boxes.shape[0]))
    for i in range(overlaps.shape[1]):
        gt = gt_boxes[i]
        overlaps[:, i] = utils.compute_iou(gt, rpn_rois, gt_box_area[i], rpn_roi_area)

    # Assign ROIs to GT boxes
    rpn_roi_iou_argmax = np.argmax(overlaps, axis=1)
    rpn_roi_iou_max = overlaps[np.arange(overlaps.shape[0]), rpn_roi_iou_argmax]
    # GT box assigned to each ROI
    rpn_roi_gt_boxes = gt_boxes[rpn_roi_iou_argmax]
    rpn_roi_gt_class_ids = gt_class_ids[rpn_roi_iou_argmax]

    # Positive ROIs are those with >= 0.5 IoU with a GT box.
    fg_ids = np.where(rpn_roi_iou_max > 0.5)[0]

    # Negative ROIs are those with max IoU 0.1-0.5 (hard example mining)
    # TODO: To hard example mine or not to hard example mine, that's the question
    # bg_ids = np.where((rpn_roi_iou_max >= 0.1) & (rpn_roi_iou_max < 0.5))[0]
    bg_ids = np.where(rpn_roi_iou_max < 0.5)[0]

    # Subsample ROIs. Aim for 33% foreground.
    # FG
    fg_roi_count = int(config.TRAIN_ROIS_PER_IMAGE * config.ROI_POSITIVE_RATIO)
    if fg_ids.shape[0] > fg_roi_count:
        keep_fg_ids = np.random.choice(fg_ids, fg_roi_count, replace=False)
        #### balance random sampling of each class ID ### added by Marzi
    #         for

    else:
        keep_fg_ids = fg_ids
    # BG
    remaining = config.TRAIN_ROIS_PER_IMAGE - keep_fg_ids.shape[0]
    if bg_ids.shape[0] > remaining:
        keep_bg_ids = np.random.choice(bg_ids, remaining, replace=False)
    else:
        keep_bg_ids = bg_ids
    # Combine indices of ROIs to keep
    keep = np.concatenate([keep_fg_ids, keep_bg_ids])
    # Need more?
    remaining = config.TRAIN_ROIS_PER_IMAGE - keep.shape[0]
    if remaining > 0:
        # Looks like we don't have enough samples to maintain the desired
        # balance. Reduce requirements and fill in the rest. This is
        # likely different from the Mask RCNN paper.

        # There is a small chance we have neither fg nor bg samples.
        if keep.shape[0] == 0:
            # Pick bg regions with easier IoU threshold
            bg_ids = np.where(rpn_roi_iou_max < 0.5)[0]
            assert bg_ids.shape[0] >= remaining
            keep_bg_ids = np.random.choice(bg_ids, remaining, replace=False)
            assert keep_bg_ids.shape[0] == remaining
            keep = np.concatenate([keep, keep_bg_ids])
        else:
            # Fill the rest with repeated bg rois.
            keep_extra_ids = np.random.choice(keep_bg_ids, remaining, replace=True)
            keep = np.concatenate([keep, keep_extra_ids])
    assert (
        keep.shape[0] == config.TRAIN_ROIS_PER_IMAGE
    ), "keep doesn't match ROI batch size {}, {}".format(
        keep.shape[0], config.TRAIN_ROIS_PER_IMAGE
    )

    # Reset the gt boxes assigned to BG ROIs.
    rpn_roi_gt_boxes[keep_bg_ids, :] = 0
    rpn_roi_gt_class_ids[keep_bg_ids] = 0

    # For each kept ROI, assign a class_id, and for FG ROIs also add bbox refinement.
    rois = rpn_rois[keep]
    roi_gt_boxes = rpn_roi_gt_boxes[keep]
    roi_gt_class_ids = rpn_roi_gt_class_ids[keep]
    roi_gt_assignment = rpn_roi_iou_argmax[keep]

    # Class-aware bbox deltas. [y, x, log(h), log(w)]
    bboxes = np.zeros(
        (config.TRAIN_ROIS_PER_IMAGE, config.NUM_CLASSES, 4), dtype=np.float32
    )
    pos_ids = np.where(roi_gt_class_ids > 0)[0]
    bboxes[pos_ids, roi_gt_class_ids[pos_ids]] = utils.box_refinement(
        rois[pos_ids], roi_gt_boxes[pos_ids, :4]
    )
    # Normalize bbox refinements
    bboxes /= config.BBOX_STD_DEV

    # Generate class-specific target masks
    masks = np.zeros(
        (
            config.TRAIN_ROIS_PER_IMAGE,
            config.MASK_SHAPE[0],
            config.MASK_SHAPE[1],
            config.NUM_CLASSES,
        ),
        dtype=np.float32,
    )
    for i in pos_ids:
        class_id = roi_gt_class_ids[i]
        assert class_id > 0, "class id must be greater than 0"
        gt_id = roi_gt_assignment[i]
        class_mask = gt_masks[:, :, gt_id]

        if config.USE_MINI_MASK:
            # Create a mask placeholder, the size of the image
            placeholder = np.zeros(config.IMAGE_SHAPE[:2], dtype=bool)
            # GT box
            gt_y1, gt_x1, gt_y2, gt_x2 = gt_boxes[gt_id]
            gt_w = gt_x2 - gt_x1
            gt_h = gt_y2 - gt_y1
            # Resize mini mask to size of GT box
            placeholder[gt_y1:gt_y2, gt_x1:gt_x2] = np.round(
                utils.resize(class_mask, (gt_h, gt_w))
            ).astype(bool)
            # Place the mini batch in the placeholder
            class_mask = placeholder

        # Pick part of the mask and resize it
        y1, x1, y2, x2 = rois[i].astype(np.int32)
        m = class_mask[y1:y2, x1:x2]
        mask = utils.resize(m, config.MASK_SHAPE)
        masks[i, :, :, class_id] = mask

    return rois, roi_gt_class_ids, bboxes, masks


def build_rpn_targets_marzi(image_shape, anchors, gt_class_ids, gt_boxes, config):
    """Given the anchors and GT boxes, compute overlaps and identify positive
    anchors and deltas to refine them to match their corresponding GT boxes.
    anchors: [num_anchors, (y1, x1, y2, x2)]
    gt_class_ids: [num_gt_boxes] Integer class IDs.
    gt_boxes: [num_gt_boxes, (y1, x1, y2, x2)]
    Returns:
    rpn_match: [N] (int32) matches between anchors and GT boxes.
               1 = positive anchor, -1 = negative anchor, 0 = neutral
    rpn_bbox: [config.RPN_TRAIN_ANCHORS_PER_IMAGE, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.
    """
    # RPN Match: 1 = positive anchor, -1 = negative anchor, 0 = neutral
    rpn_match = np.zeros([anchors.shape[0]], dtype=np.int32)
    rpn_match_class = np.zeros([anchors.shape[0]], dtype=np.int32)
    # RPN bounding boxes: [max anchors per image, (dy, dx, log(dh), log(dw))]
    #     rpn_bbox = np.zeros((config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4))

    # Handle COCO crowds
    # A crowd box in COCO is a bounding box around several instances. Exclude
    # them from training. A crowd box is given a negative class ID.
    crowd_ix = np.where(gt_class_ids < 0)[0]
    if crowd_ix.shape[0] > 0:
        # Filter out crowds from ground truth class IDs and boxes
        non_crowd_ix = np.where(gt_class_ids > 0)[0]
        crowd_boxes = gt_boxes[crowd_ix]
        gt_class_ids = gt_class_ids[non_crowd_ix]
        gt_boxes = gt_boxes[non_crowd_ix]
        # Compute overlaps with crowd boxes [anchors, crowds]
        crowd_overlaps = utils.compute_overlaps(anchors, crowd_boxes)
        crowd_iou_max = np.amax(crowd_overlaps, axis=1)
        no_crowd_bool = crowd_iou_max < 0.001
    else:
        # All anchors don't intersect a crowd
        no_crowd_bool = np.ones([anchors.shape[0]], dtype=bool)

    # Compute overlaps [num_anchors, num_gt_boxes]

    overlaps = utils.compute_overlaps(anchors, gt_boxes)
    #     gt_class_ids_tiled=np.tile(gt_class_ids,(anchors.shape[0],1))

    #     print('overlaps.shape',overlaps.shape,gt_class_ids_tiled.shape)

    # Match anchors to GT Boxes
    # If an anchor overlaps a GT box with IoU >= 0.7 then it's positive.
    # If an anchor overlaps a GT box with IoU < 0.3 then it's negative.
    # Neutral anchors are those that don't match the conditions above,
    # and they don't influence the loss function.
    # However, don't keep any GT box unmatched (rare, but happens). Instead,
    # match it to the closest anchor (even if its max IoU is < 0.3).
    #
    # 1. Set negative anchors first. They get overwritten below if a GT box is
    # matched to them. Skip boxes in crowd areas.
    anchor_iou_argmax = np.argmax(overlaps, axis=1)
    anchor_iou_max = overlaps[np.arange(overlaps.shape[0]), anchor_iou_argmax]

    #     anchor_iou_max_class = gt_class_ids_tiled[np.arange(overlaps.shape[0]), anchor_iou_argmax]
    anchor_iou_max_class = gt_class_ids[anchor_iou_argmax]

    rpn_match[(anchor_iou_max < 0.3) & (no_crowd_bool)] = -1
    #     rpn_match[(anchor_iou_max ==0) & (no_crowd_bool)] = -1

    # 2. Set an anchor for each GT box (regardless of IoU value).
    # If multiple anchors have the same IoU match all of them
    gt_iou_argmax = np.argwhere(overlaps == np.max(overlaps, axis=0))[:, 0]
    rpn_match[gt_iou_argmax] = 1
    rpn_match_class[gt_iou_argmax] = anchor_iou_max_class[gt_iou_argmax]
    # 3. Set anchors with high overlap as positive.
    rpn_match[anchor_iou_max >= 0.7] = 1
    #     rpn_match[anchor_iou_max >= 0.2] = 1

    pos_ids = np.where(rpn_match == 1)[0]

    neg_ids = np.where(rpn_match == -1)[0]
    #     print(neg_ids.shape[0],pos_ids.shape[0])
    if neg_ids.shape[0] > pos_ids.shape[0]:
        #     hgfjh
        ids = np.random.choice(
            neg_ids, neg_ids.shape[0] - pos_ids.shape[0], replace=False
        )
        rpn_match[ids] = 0

    rpn_bbox = np.zeros((pos_ids.shape[0], 4))
    # For positive anchors, compute shift and scale needed to transform them
    # to match the corresponding GT boxes.
    ids = np.where(rpn_match == 1)[0]
    ix = 0  # index into rpn_bbox
    # TODO: use box_refinement() rather than duplicating the code here
    for i, a in zip(ids, anchors[ids]):
        # Closest gt box (it might have IoU < 0.7)
        gt = gt_boxes[anchor_iou_argmax[i]]

        # Convert coordinates to center plus width/height.
        # GT Box
        gt_h = gt[2] - gt[0]
        gt_w = gt[3] - gt[1]
        gt_center_y = gt[0] + 0.5 * gt_h
        gt_center_x = gt[1] + 0.5 * gt_w
        # Anchor
        a_h = a[2] - a[0]
        a_w = a[3] - a[1]
        a_center_y = a[0] + 0.5 * a_h
        a_center_x = a[1] + 0.5 * a_w

        # Compute the bbox refinement that the RPN should predict.
        rpn_bbox[ix] = [
            (gt_center_y - a_center_y) / a_h,
            (gt_center_x - a_center_x) / a_w,
            np.log(gt_h / a_h),
            np.log(gt_w / a_w),
        ]
        # Normalize
        rpn_bbox[ix] /= config.RPN_BBOX_STD_DEV
        ix += 1

    return rpn_match, rpn_bbox, rpn_match_class


def build_rpn_targets(image_shape, anchors, gt_class_ids, gt_boxes, config):
    """Given the anchors and GT boxes, compute overlaps and identify positive
    anchors and deltas to refine them to match their corresponding GT boxes.
    anchors: [num_anchors, (y1, x1, y2, x2)]
    gt_class_ids: [num_gt_boxes] Integer class IDs.
    gt_boxes: [num_gt_boxes, (y1, x1, y2, x2)]
    Returns:
    rpn_match: [N] (int32) matches between anchors and GT boxes.
               1 = positive anchor, -1 = negative anchor, 0 = neutral
    rpn_bbox: [config.RPN_TRAIN_ANCHORS_PER_IMAGE, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.
    """
    # RPN Match: 1 = positive anchor, -1 = negative anchor, 0 = neutral
    rpn_match = np.zeros([anchors.shape[0]], dtype=np.int32)
    rpn_match_class = np.zeros([anchors.shape[0]], dtype=np.int32)
    # RPN bounding boxes: [max anchors per image, (dy, dx, log(dh), log(dw))]
    rpn_bbox = np.zeros((config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4))

    # Handle COCO crowds
    # A crowd box in COCO is a bounding box around several instances. Exclude
    # them from training. A crowd box is given a negative class ID.
    crowd_ix = np.where(gt_class_ids < 0)[0]
    if crowd_ix.shape[0] > 0:
        # Filter out crowds from ground truth class IDs and boxes
        non_crowd_ix = np.where(gt_class_ids > 0)[0]
        crowd_boxes = gt_boxes[crowd_ix]
        gt_class_ids = gt_class_ids[non_crowd_ix]
        gt_boxes = gt_boxes[non_crowd_ix]
        # Compute overlaps with crowd boxes [anchors, crowds]
        crowd_overlaps = utils.compute_overlaps(anchors, crowd_boxes)
        crowd_iou_max = np.amax(crowd_overlaps, axis=1)
        no_crowd_bool = crowd_iou_max < 0.001
    else:
        # All anchors don't intersect a crowd
        no_crowd_bool = np.ones([anchors.shape[0]], dtype=bool)

    # Compute overlaps [num_anchors, num_gt_boxes]

    overlaps = utils.compute_overlaps(anchors, gt_boxes)
    #     gt_class_ids_tiled=np.tile(gt_class_ids,(anchors.shape[0],1))

    #     print('overlaps.shape',overlaps.shape,gt_class_ids_tiled.shape)

    # Match anchors to GT Boxes
    # If an anchor overlaps a GT box with IoU >= 0.7 then it's positive.
    # If an anchor overlaps a GT box with IoU < 0.3 then it's negative.
    # Neutral anchors are those that don't match the conditions above,
    # and they don't influence the loss function.
    # However, don't keep any GT box unmatched (rare, but happens). Instead,
    # match it to the closest anchor (even if its max IoU is < 0.3).
    #
    # 1. Set negative anchors first. They get overwritten below if a GT box is
    # matched to them. Skip boxes in crowd areas.
    anchor_iou_argmax = np.argmax(overlaps, axis=1)
    anchor_iou_max = overlaps[np.arange(overlaps.shape[0]), anchor_iou_argmax]

    #     anchor_iou_max_class = gt_class_ids_tiled[np.arange(overlaps.shape[0]), anchor_iou_argmax]
    anchor_iou_max_class = gt_class_ids[anchor_iou_argmax]

    rpn_match[(anchor_iou_max < 0.3) & (no_crowd_bool)] = -1
    # 2. Set an anchor for each GT box (regardless of IoU value).
    # If multiple anchors have the same IoU match all of them
    gt_iou_argmax = np.argwhere(overlaps == np.max(overlaps, axis=0))[:, 0]
    rpn_match[gt_iou_argmax] = 1
    rpn_match_class[gt_iou_argmax] = anchor_iou_max_class[gt_iou_argmax]
    # 3. Set anchors with high overlap as positive.
    rpn_match[anchor_iou_max >= 0.7] = 1
    #     anchor_iou_max_class[anchor_iou_max < 0.7] = 0

    # Subsample to balance positive and negative anchors
    # Don't let positives be more than half the anchors
    ids = np.where(rpn_match == 1)[0]
    extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE // 2)
    if extra > 0:
        # Reset the extra ones to neutral
        ids = np.random.choice(ids, extra, replace=False)
        rpn_match[ids] = 0
    # Same for negative proposals
    ids = np.where(rpn_match == -1)[0]
    extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE - np.sum(rpn_match == 1))
    if extra > 0:
        # Rest the extra ones to neutral
        ids = np.random.choice(ids, extra, replace=False)
        rpn_match[ids] = 0

    # For positive anchors, compute shift and scale needed to transform them
    # to match the corresponding GT boxes.
    ids = np.where(rpn_match == 1)[0]
    ix = 0  # index into rpn_bbox
    # TODO: use box_refinement() rather than duplicating the code here
    for i, a in zip(ids, anchors[ids]):
        # Closest gt box (it might have IoU < 0.7)
        gt = gt_boxes[anchor_iou_argmax[i]]

        # Convert coordinates to center plus width/height.
        # GT Box
        gt_h = gt[2] - gt[0]
        gt_w = gt[3] - gt[1]
        gt_center_y = gt[0] + 0.5 * gt_h
        gt_center_x = gt[1] + 0.5 * gt_w
        # Anchor
        a_h = a[2] - a[0]
        a_w = a[3] - a[1]
        a_center_y = a[0] + 0.5 * a_h
        a_center_x = a[1] + 0.5 * a_w

        # Compute the bbox refinement that the RPN should predict.
        rpn_bbox[ix] = [
            (gt_center_y - a_center_y) / a_h,
            (gt_center_x - a_center_x) / a_w,
            np.log(gt_h / a_h),
            np.log(gt_w / a_w),
        ]
        # Normalize
        rpn_bbox[ix] /= config.RPN_BBOX_STD_DEV
        ix += 1

    return rpn_match, rpn_bbox, rpn_match_class


def generate_random_rois(image_shape, count, gt_class_ids, gt_boxes):
    """Generates ROI proposals similar to what a region proposal network
    would generate.

    image_shape: [Height, Width, Depth]
    count: Number of ROIs to generate
    gt_class_ids: [N] Integer ground truth class IDs
    gt_boxes: [N, (y1, x1, y2, x2)] Ground truth boxes in pixels.

    Returns: [count, (y1, x1, y2, x2)] ROI boxes in pixels.
    """
    # placeholder
    rois = np.zeros((count, 4), dtype=np.int32)

    # Generate random ROIs around GT boxes (90% of count)
    rois_per_box = int(0.9 * count / gt_boxes.shape[0])
    for i in range(gt_boxes.shape[0]):
        gt_y1, gt_x1, gt_y2, gt_x2 = gt_boxes[i]
        h = gt_y2 - gt_y1
        w = gt_x2 - gt_x1
        # random boundaries
        r_y1 = max(gt_y1 - h, 0)
        r_y2 = min(gt_y2 + h, image_shape[0])
        r_x1 = max(gt_x1 - w, 0)
        r_x2 = min(gt_x2 + w, image_shape[1])

        # To avoid generating boxes with zero area, we generate double what
        # we need and filter out the extra. If we get fewer valid boxes
        # than we need, we loop and try again.
        while True:
            y1y2 = np.random.randint(r_y1, r_y2, (rois_per_box * 2, 2))
            x1x2 = np.random.randint(r_x1, r_x2, (rois_per_box * 2, 2))
            # Filter out zero area boxes
            threshold = 1
            y1y2 = y1y2[np.abs(y1y2[:, 0] - y1y2[:, 1]) >= threshold][:rois_per_box]
            x1x2 = x1x2[np.abs(x1x2[:, 0] - x1x2[:, 1]) >= threshold][:rois_per_box]
            if y1y2.shape[0] == rois_per_box and x1x2.shape[0] == rois_per_box:
                break

        # Sort on axis 1 to ensure x1 <= x2 and y1 <= y2 and then reshape
        # into x1, y1, x2, y2 order
        x1, x2 = np.split(np.sort(x1x2, axis=1), 2, axis=1)
        y1, y2 = np.split(np.sort(y1y2, axis=1), 2, axis=1)
        box_rois = np.hstack([y1, x1, y2, x2])
        rois[rois_per_box * i : rois_per_box * (i + 1)] = box_rois

    # Generate random ROIs anywhere in the image (10% of count)
    remaining_count = count - (rois_per_box * gt_boxes.shape[0])
    #     print(count,rois_per_box,gt_boxes.shape[0],"remaining_count",remaining_count)
    # To avoid generating boxes with zero area, we generate double what
    # we need and filter out the extra. If we get fewer valid boxes
    # than we need, we loop and try again.
    while True:
        y1y2 = np.random.randint(0, image_shape[0], (remaining_count * 2, 2))
        x1x2 = np.random.randint(0, image_shape[1], (remaining_count * 2, 2))
        # Filter out zero area boxes
        threshold = 1
        y1y2 = y1y2[np.abs(y1y2[:, 0] - y1y2[:, 1]) >= threshold][:remaining_count]
        x1x2 = x1x2[np.abs(x1x2[:, 0] - x1x2[:, 1]) >= threshold][:remaining_count]
        if y1y2.shape[0] == remaining_count and x1x2.shape[0] == remaining_count:
            break

    # Sort on axis 1 to ensure x1 <= x2 and y1 <= y2 and then reshape
    # into x1, y1, x2, y2 order
    x1, x2 = np.split(np.sort(x1x2, axis=1), 2, axis=1)
    y1, y2 = np.split(np.sort(y1y2, axis=1), 2, axis=1)
    global_rois = np.hstack([y1, x1, y2, x2])
    rois[-remaining_count:] = global_rois
    return rois


############################################################
#  Data Formatting
############################################################


def compose_image_meta(
    image_id,
    original_image_shape,
    image_shape,
    window,
    scale,
    active_class_ids,
    img_site_and_epoch,
):
    """Takes attributes of an image and puts them in one 1D array.

    image_id: An int ID of the image. Useful for debugging.
    original_image_shape: [H, W, C] before resizing or padding.
    image_shape: [H, W, C] after resizing and padding
    window: (y1, x1, y2, x2) in pixels. The area of the image where the real
            image is (excluding the padding)
    scale: The scaling factor applied to the original image (float32)
    active_class_ids: List of class_ids available in the dataset from which
        the image came. Useful if training on images from multiple datasets
        where not all classes are present in all datasets.
    img_site_and_epoch:

    """
    meta = np.array(
        [image_id]
        + list(original_image_shape)  # size=1
        + list(image_shape)  # size=3
        + list(window)  # size=3
        + [scale]  # size=4 (y1, x1, y2, x2) in image cooredinates
        + list(active_class_ids)  # size=1
        + img_site_and_epoch  # size=num_classes
    )
    return meta


def parse_image_meta(meta):
    """Parses an array that contains image attributes to its components.
    See compose_image_meta() for more details.

    meta: [batch, meta length] where meta length depends on NUM_CLASSES

    Returns a dict of the parsed values.
    """
    image_id = meta[:, 0]
    original_image_shape = meta[:, 1:4]
    image_shape = meta[:, 4:7]
    window = meta[:, 7:11]  # (y1, x1, y2, x2) window of image in in pixels
    scale = meta[:, 11]
    active_class_ids = meta[:, 12:-2]
    im_site_epoch = meta[:, -2]
    return {
        "image_id": image_id.astype(np.int32),
        "original_image_shape": original_image_shape.astype(np.int32),
        "image_shape": image_shape.astype(np.int32),
        "window": window.astype(np.int32),
        "scale": scale.astype(np.float32),
        "active_class_ids": active_class_ids.astype(np.int32),
        "im_site_epoch": im_site_epoch.astype(np.int32),
    }


def parse_image_meta_graph(meta):
    """Parses a tensor that contains image attributes to its components.
    See compose_image_meta() for more details.

    meta: [batch, meta length] where meta length depends on NUM_CLASSES

    Returns a dict of the parsed tensors.
    """
    image_id = meta[:, 0]
    original_image_shape = meta[:, 1:4]
    image_shape = meta[:, 4:7]
    window = meta[:, 7:11]  # (y1, x1, y2, x2) window of image in in pixels
    scale = meta[:, 11]
    active_class_ids = meta[:, 12:-2]
    im_site_epoch = meta[:, -2]
    #     print("active_class_ids",active_class_ids)
    return {
        "image_id": image_id,
        "original_image_shape": original_image_shape,
        "image_shape": image_shape,
        "window": window,
        "scale": scale,
        "active_class_ids": active_class_ids,
        "im_site_epoch": im_site_epoch,
    }


def mold_image(images, config):
    """Expects an RGB image (or array of images) and subtracts
    the mean pixel and converts it to float. Expects image
    colors in RGB order.
    """
    return images.astype(np.float32) - config.MEAN_PIXEL


def unmold_image(normalized_images, config):
    """Takes a image normalized with mold() and returns the original."""
    return (normalized_images + config.MEAN_PIXEL).astype(np.uint8)


def generate_gt_rois(image_shape, count, gt_class_ids, gt_boxes):
    """Generates ROI proposals converting GT boxes into proposaly. use same output
    format as RPN network layer has. We add one backgroung proposals. This is
    used later to fill up the proposals list

    image_shape: [Height, Width, Depth]
    gt_class_ids: [N] Integer ground truth class IDs
    gt_boxes: [N, (y1, x1, y2, x2)] Ground truth boxes in pixels.
    mode: only inference mode will be accepted
    Returns: [count, (y1, x1, y2, x2)] ROI boxes in pixels.
    """

    # placeholder
    rois = np.zeros((count, 4), dtype=np.int32)

    # Simply copy ground truth boxes over to rois boxes as foregrounds
    for i in range(gt_boxes.shape[0]):
        gt_y1, gt_x1, gt_y2, gt_x2 = gt_boxes[i]
        box_rois = np.hstack([gt_y1, gt_x1, gt_y2, gt_x2])
        rois[i] = box_rois

    # We add up the rest with background item, very small. These
    # proposals will later fall out anyway.
    for i in range(gt_boxes.shape[0], count):
        back_rois = np.hstack([0, 0, 5, 5])
        rois[i] = back_rois

    return rois